{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Split and Bias and Variance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives\n",
    "*By the end of this notebook you will be able to:*\n",
    "- Define error due to bias and error due to variance.\n",
    "- Identify the bias-variance trade-off.\n",
    "- Describe what overfitting and underfitting means in the context of model building.\n",
    "- Explain problems associated with over- and underfitting.\n",
    "- Grasp why train/test split is necessary.\n",
    "- Explore k-folds, LOOCV, and three split methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bias-and-variance-trade-off\"></a>\n",
    "# <font color='blue'> Bias and variance tradeoff\n",
    "---\n",
    "The **bias-variance tradeoff** is widely used in machine learning as a conceptual way of comparing and contrasting different models. \n",
    "\n",
    "**Bias: How close are predictions to the actual values?** \n",
    "\n",
    "* Bias is error stemming from incorrect model assumptions\n",
    "* If the model cannot represent the data's structure, our predictions could be consistent, but will not be accurate.\n",
    "* **Example**: Assuming data is linear when it has a more complicated structure.\n",
    "\n",
    "**Variance:  How variable are our predictions?** \n",
    "* Variance is error stemming from being overly sensitive to changes in the training data.\n",
    "* A model with high variance will make very different predictions given slightly different training sets.\n",
    "* **Example**: Using the training set exactly (e.g. 1-NN) for a model results in a completely different model -- even if the training set differs only slightly.\n",
    "\n",
    "\n",
    "As model complexity **increases**:\n",
    "- Bias **decreases**. (The model can more accurately model complex structure in data.)\n",
    "- Variance **increases**. (The model identifies more complex structures, making it more sensitive to small changes in the training data.)\n",
    "\n",
    "![](./assets/biasVsVarianceImage.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exploring-the-bias-variance-tradeoff\"></a>\n",
    "## <font color='red'> Now you try: High bias, low variance\n",
    "    \n",
    "Let's try building models with high variance/low bias, and low variance/high bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Allow plots to appear in the notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's read in the mammals dataset.\n",
    "\n",
    "This is a [data set](http://people.sc.fsu.edu/~jburkardt/datasets/regression/x01.txt) of the average weight of the body (in kg) and the brain (in g) for 62 mammal species. \n",
    "\n",
    "We'll use this dataset to investigate bias vs. variance. Let's read it into Pandas and take a quick look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/mammals.txt'\n",
    "cols = ['brain','body']\n",
    "mammals = pd.read_table(path, sep='\\t', names=cols, header=0)\n",
    "mammals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mammals.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep rows in which the body weight is less than 200 kg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We're now going to pretend that there are only 51 mammal species in existence. In other words, we are pretending that this is the entire data set of brain and body weights for **every known mammal species**.\n",
    "\n",
    "Let's create a scatterplot (using [Seaborn](http://stanford.edu/~mwaskom/software/seaborn/)'s `lmplot` function ) to visualize the relationship between brain and body weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, fit_reg=False);\n",
    "sns.plt.xlim(-10, 200);\n",
    "sns.plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There appears to be a relationship between brain and body weight for mammals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's pretend that a **new mammal species** is discovered. We measure the body weight of every member of this species we can find and calculate an **average body weight of 100 kgs**. We want to **predict the average brain weight** of this species (rather than measuring it directly). How might we do this?\n",
    "\n",
    "`lmplot` has an option that allows us to draw a regression line over our data. This is a useful, quick visualisation tool. We do this by leaving out the parameter `fit_reg=False`; the default value for this parameter is `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None);\n",
    "plt.xlim(-10, 200);\n",
    "plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We drew a straight line that appears to best capture the relationship between brain and body weight. So, we might predict that our new species has a brain weight of about 45 g, as that's the approximate y value when x=100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Earlier, we assumed that this dataset contained every known mammal species. That's very convenient, but **in the real world, all you ever have is a sample of data**. This may sound like a contentious statement, but the point of machine learning is to generalize from a sample to the population. If you already have data for the entire population, then you have no need for machine learning -- you can apply statistics directly and get optimal answers!\n",
    "\n",
    "Here, a more realistic situation would be to only have brain and body weights for (let's say) half of the 51 known mammals.\n",
    "\n",
    "When that new mammal species (with a body weight of 100 kg) is discovered, we still want to make an accurate prediction for its brain weight, but this task might be more difficult, as we don't have all of the data we would ideally like to have.\n",
    "\n",
    "Let's simulate this situation by assigning each of the 51 observations to **either universe 1 or universe 2**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility.\n",
    "np.random.seed(12345)\n",
    "\n",
    "# Randomly assign every observation to either universe 1 or universe 2.\n",
    "mammals['universe'] = np.random.randint(1, 3, len(mammals))\n",
    "mammals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Important:** We only live in one of the two universes. Both universes have 51 known mammal species, but each universe knows the brain and body weight for different species.\n",
    "\n",
    "We can now tell Seaborn to create two plots in which the left plot only uses the data from **universe 1** and the right plot only uses the data from **universe 2**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col='universe' subsets the data by universe and creates two separate plots.\n",
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, col='universe');\n",
    "plt.xlim(-10, 200);\n",
    "plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The line looks pretty similar between the two plots, despite the fact that they used separate samples of data. In both cases, we would predict a brain weight of about 45 g.\n",
    "\n",
    "It's easier to see the degree of similarity by placing them on the same plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hue='universe' subsets the data by universe and creates a single plot.\n",
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, hue='universe');\n",
    "plt.xlim(-10, 200);\n",
    "plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So, what was the point of this exercise? This was a visual demonstration of a high-bias, low-variance model.\n",
    "\n",
    "- It's **high bias** because it doesn't fit the data particularly well.\n",
    "- It's **low variance** because it doesn't change much depending on which observations happen to be available in that universe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"lets-try-something-completely-different\"></a>\n",
    "## <font color='red'> Now you try: Low bias, high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What would a **low-bias, high-variance** model look like? Let's try polynomial regression with an eighth-order polynomial. Hint: You'll need `lmplot` again but with the parameter `order` to set the degree of the polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- It's **low bias** because the models match the data effectively.\n",
    "- It's **high variance** because the models are widely different, depending on which observations happen to be available in that universe. (For a body weight of 100 kg, the brain weight prediction would be 40 kg in one universe and 0 kg in the other!)\n",
    "\n",
    "Now, try plotting the two models on the same set of axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"balancing-bias-and-variance\"></a>\n",
    "## <font color='red'> Now you try: Balancing bias and variance\n",
    "    \n",
    "Can we find a middle ground?\n",
    "\n",
    "Perhaps we can create a model that has **less bias than the linear model** and **less variance than the eighth order polynomial**?\n",
    "\n",
    "Try fitting a second order polynomial to the data instead, and plotting the two models on separate axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This seems better. In both the left and right plots, **it fits the data well, but not too well**.\n",
    "\n",
    "This is the essence of the **bias-variance trade-off**: You are seeking a model that appropriately balances bias and variance and thus will generalize to new data (known as \"out-of-sample\" data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We want a model that best balances bias and variance. It\n",
    "should match our training data well (moderate bias) yet be low variance for out-of-sample data (moderate variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Training error as a function of\n",
    "complexity.\n",
    "- Question: Why do we even\n",
    "care about variance if we\n",
    "know we can generate a\n",
    "more accurate model with\n",
    "higher complexity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we obtain a zero-bias, zero-variance model?\n",
    "\n",
    "No! If there is any noise in the data-generating process, then a zero-variance model would not be learning from the data. Additionally, a model only has zero bias if the true relationship between the target and the features is hard-coded into it. If that were the case, you wouldn't be doing machine learning -- it would be similar to trying to predict today's temperature by using today's temperature!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"train-test-split\"></a>\n",
    "# <font color='blue'> Training and testing split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of the lab, we will look at three evaluation procedures for predicting model out-of-sample accuracy. \n",
    "\n",
    "\n",
    "1. **Train-test-split** is useful if cross-validation is not practical (e.g. it takes too long to train). It is also useful for computing a quick confusion matrix. You could also use this as a final step after the model is finalized (often called evaluating the model against a **validation set**).\n",
    "\n",
    "\n",
    "2. **Cross-validation** is the gold standard for estimating accuracy and comparing accuracy across models.\n",
    "\n",
    "\n",
    "\n",
    "3. **Three-way split** combines cross-validation and the train-test-split. It takes an initial split to be used as a final validation set, then uses cross-validation on the rest.\n",
    "\n",
    "\n",
    "We'll be running a linear regression model to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train-test-split-1\"></a>\n",
    "## <font color='red'> Now you try: Training and testing split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by reading in the Boston housing dataset.\n",
    "\n",
    "Drop rows containing missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv('./data/boston_housing.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, scale each **feature** to fall between 0 and 1. This means:\n",
    "\n",
    "\n",
    "* It will be possible to compare the relative importances of the different features by interpreting their coefficients.\n",
    "\n",
    "\n",
    "* It won't be possible to interpret the coefficients in a real-world context, i.e. we won't be able to make statements like \"for every extra bedroom added to a house, its value increases by $1000\".\n",
    "\n",
    "**You only want to scale the feature columns, not the target**\n",
    "\n",
    "The `MinMaxScaler` from `scikit-learn` will be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "cols_to_scale = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','LSTAT']\n",
    "\n",
    "scaler = \n",
    "\n",
    "housing_scaled = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, check which variables are normally distributed. You can do this using `hist` and adjusting the `figsize` parameter so you can see all the variables clearly. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, remove variables that aren't normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into a training and testing set, where the testing set is 30% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = housing_scaled[['INDUS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','LSTAT']]\n",
    "y = housing['MEDV']\n",
    "\n",
    "X_train, X_test, y_train, y_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test a linear regression model on this data; compare its performance to a baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"k-folds-cross-validation\"></a>\n",
    "## K-fold cross validation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Train/test split provides us with helpful tool, but it's a shame that we are tossing out a large chunk of our data for testing purposes.\n",
    "\n",
    "**How can we use the maximum amount of our data points while still ensuring model integrity?**\n",
    "\n",
    "1. Split our data into a number of different pieces (folds).\n",
    "2. Train using `k-1` folds for training and a different fold for testing.\n",
    "3. Average our model against EACH of those iterations.\n",
    "4. Choose our model and TEST it against the final fold.\n",
    "5. Average all test accuracies to get the estimated out-of-sample accuracy.\n",
    "\n",
    "Although this may sound complicated, we are just training the model on k separate train-test-splits, then taking the average of the resulting test accuracies!\n",
    "\n",
    "<a id=\"leave-one-out-cross-validation\"></a>\n",
    "### Leave-One-Out Cross-Validation\n",
    "\n",
    "A special case of k-fold cross-validation is leave-one-out cross-validation. Rather than taking 5–10 folds, we take a fold of size `n-1` and leave one observation to test. \n",
    "\n",
    "Typically, 5–10 fold cross-validaiton is recommended.\n",
    "\n",
    "Let's see how k-fold cross validation works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "linreg = LinearRegression()\n",
    "scores = cross_val_score(linreg, X, y, cv=5,scoring='neg_mean_squared_error')\n",
    "\n",
    "scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> K-fold cross validation\n",
    "    \n",
    "1. Can you convert the scores above into positive values of root mean squared error? \n",
    "\n",
    "\n",
    "2. What's the mean error across five folds?\n",
    "\n",
    "\n",
    "Now, let's try k-fold cross validation on our US Presidential data.\n",
    "\n",
    "\n",
    "1. Read in the dataset. \n",
    "2. Drop rows containing missing values.\n",
    "3. Normalise each feature column so the values all fall between 0 and 1\n",
    "4. Remove features that aren't normally distributed\n",
    "5. Using linear regression models and 10-fold cross validation, compute the root mean squared errors of all 10 models. T\n",
    "6. Find the mean error across all 10 models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"three-way-data-split\"></a>\n",
    "## Three-Way Data Split\n",
    "---\n",
    "\n",
    "The most common workflow is actually a combination of train/test split and cross-validation. \n",
    "\n",
    "We take a train/test split on our data right away.\n",
    "\n",
    "We then take our training data and tune our models using cross-validation. \n",
    "\n",
    "When we think we are done, we do one last test on the testing data to make sure we haven't accidently overfit to our training data.\n",
    "\n",
    "**If you tune hyperparameters via cross-validation, you should never use cross-validation on the same dataset to estimate OOS accuracy!** \n",
    "\n",
    "Using cross-validation in this way, the entire dataset is used to tune hyperparameters. So, this invalidates our condition above -- where we assumed the test set is a pretend \"out-of-sample\" dataset that was not used to train our model! So, we would expect the accuracy on this test set to be artificially inflated as compared to actual \"out-of-sample\" data.\n",
    "\n",
    "Even with good evaluation procedures, it is incredible easy to overfit our models by including features that will not be available during production or leak information about our testing data in other ways."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
