{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Natural Language Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "*After completing this notebook, you will be able to:*\n",
    "- Discuss the major tasks involved with natural language processing.\n",
    "- Discuss, on a low level, the components of natural language processing.\n",
    "- Identify why natural language processing is difficult.\n",
    "- Demonstrate text classification.\n",
    "- Demonstrate common text preprocessing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Now you try\n",
    "    \n",
    "Let's read in the dataset for this session.\n",
    "\n",
    "Run the cells below and work out what each row and column corresponds to.\n",
    "\n",
    "Then run the final cell to filter the dataframe to include **only** reviews with **1 or 5 stars**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB         # Naive Bayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read yelp.csv into a DataFrame.\n",
    "yelp = pd.read_csv('./data/yelp.csv')\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>zp713qNhx8d9KCJJnrw1xA</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>riFQ3vxNpP4rWLk_CSri2A</td>\n",
       "      <td>5</td>\n",
       "      <td>Drop what you're doing and drive here. After I...</td>\n",
       "      <td>review</td>\n",
       "      <td>wFweIWhv2fREZV_dYkz_1g</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "6  zp713qNhx8d9KCJJnrw1xA  2010-02-12  riFQ3vxNpP4rWLk_CSri2A      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "6  Drop what you're doing and drive here. After I...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  \n",
       "6  wFweIWhv2fREZV_dYkz_1g     7       7      4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new DataFrame that only contains the 5-star and 1-star reviews.\n",
    "yelp_best_worst = yelp[(yelp['stars']==5) | (yelp['stars']==1)]\n",
    "yelp_best_worst.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cleaning\"></a>\n",
    "\n",
    "# <font color='blue'> Cleaning\n",
    "    \n",
    "Let's start with some basic cleaning tasks; converting everything to lowercase and stripping out punctuation and special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'my wife took me here on my birthday for breakfast and it was excellent  the weather was perfect which made sitting outside overlooking their grounds an absolute pleasure  our waitress was excellent and our food arrived quickly on the semibusy saturday morning  it looked like the place fills up pretty quickly so the earlier you get here the better  do yourself a favor and get their bloody mary  it was phenomenal and simply the best ive ever had  im pretty sure they only use ingredients from their garden and blend them fresh when you order it  it was amazing  while everything on the menu looks excellent i had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious  it came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete  it was the best toast ive ever had  anyway i cant wait to go back'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_best_worst['text'] = yelp_best_worst['text'].str.lower().str.replace('[^\\w\\s]','').str.replace('\\n',' ')\n",
    "yelp_best_worst.loc[0,'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"removing-stopwords\"></a>\n",
    "\n",
    "# <font color='blue'> Removing stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What:** This process is used to remove common words that will likely appear in any text.\n",
    "- **Why:** Because common words exist in most documents, they likely only add noise to your model and should be removed.\n",
    "\n",
    "**What are stop words?**\n",
    "Stop words are some of the most common words in a language. They are used so that a sentence makes sense grammatically, such as prepositions and determiners, e.g., \"to,\" \"the,\" \"and.\" However, they are so commonly used that they are generally worthless for predicting the class of a document.  \n",
    "\n",
    "Example: \n",
    "\n",
    "> 1. Original sentence: \"The dog jumped over the fence\"  \n",
    "> 2. After stop-word removal: \"dog jumped over fence\"\n",
    "\n",
    "The fact that there is a fence and a dog jumped over it can be derived with or without stop words.\n",
    "\n",
    "NLTK has a built-in list of English stop words that we can inspect below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove stopwords from each document in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "yelp_best_worst['text_no_stopwords'] = yelp_best_worst['text'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop_words]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wife took birthday breakfast excellent weather perfect made sitting outside overlooking grounds absolute pleasure waitress excellent food arrived quickly semibusy saturday morning looked like place fills pretty quickly earlier get better favor get bloody mary phenomenal simply best ive ever im pretty sure use ingredients garden blend fresh order amazing everything menu looks excellent white truffle scrambled eggs vegetable skillet tasty delicious came 2 pieces griddled bread amazing absolutely made meal complete best toast ive ever anyway cant wait go back'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_best_worst.loc[0,'text_no_stopwords']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"stemming-lemmatization\"></a>\n",
    "\n",
    "# <font color='blue'> Stemming and lemmatization\n",
    "    \n",
    "Stemming is a crude process of removing common endings from sentences, such as \"s\", \"es\", \"ly\", \"ing\", and \"ed\".\n",
    "\n",
    "- **What:** Reduce a word to its base/stem/root form.\n",
    "- **Why:** This intelligently reduces the number of features by grouping together (hopefully) related words.\n",
    "- **Notes:**\n",
    "    - Stemming uses a simple and fast rule-based approach.\n",
    "    - Stemmed words are usually not shown to users (used for analysis/indexing).\n",
    "    - Some search engines treat words with the same stem as synonyms.\n",
    "    \n",
    "Lemmatization is a more refined process that uses specific language and grammar rules to derive the root of a word.  \n",
    "\n",
    "This is useful for words that do not share an obvious root such as \"better\" and \"best\".\n",
    "\n",
    "- **What:** Lemmatization derives the canonical form (\"lemma\") of a word.\n",
    "- **Why:** It can be better than stemming.\n",
    "- **Notes:** Uses a dictionary-based approach (slower than stemming).\n",
    "    \n",
    "**Lemmatization and Stemming Examples**\n",
    "\n",
    "|Lemmatization|Stemming|\n",
    "|-------------|---------|\n",
    "|shouted → shout|badly → bad|\n",
    "|best → good|computing → comput|\n",
    "|better → good|computed → comput|\n",
    "|good → good|wipes → wip|\n",
    "|wiping → wipe|wiped → wip|\n",
    "|hidden → hide|wiping → wip|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Let's try out stemming on some of our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wife took birthday breakfast excel weather perfect made sit outsid overlook ground absolut pleasur waitress excel food arriv quick semibusi saturday morn look like place fill pretti quick earlier get better favor get bloodi mari phenomen simpli best ive ever im pretti sure use ingredi garden blend fresh order amaz everyth menu look excel white truffl scrambl egg veget skillet tasti delici came 2 piec griddl bread amaz absolut made meal complet best toast ive ever anyway cant wait go back\n"
     ]
    }
   ],
   "source": [
    "# Initialize stemmer.\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Stem each word.\n",
    "print(' '.join([stemmer.stem(word) for word in yelp_best_worst.loc[0,'text_no_stopwords'].split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize\n",
    "\n",
    "Now let's try lemmatizing some of our documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wife', 'take', 'birthday', 'breakfast', 'excellent', 'weather', 'perfect', 'make', 'sit', 'outside', 'overlook', 'ground', 'absolute', 'pleasure', 'waitress', 'excellent', 'food', 'arrive', 'quickly', 'semibusy', 'saturday', 'morning', 'look', 'like', 'place', 'fill', 'pretty', 'quickly', 'earlier', 'get', 'better', 'favor', 'get', 'bloody', 'mary', 'phenomenal', 'simply', 'best', 'ive', 'ever', 'im', 'pretty', 'sure', 'use', 'ingredients', 'garden', 'blend', 'fresh', 'order', 'amaze', 'everything', 'menu', 'look', 'excellent', 'white', 'truffle', 'scramble', 'egg', 'vegetable', 'skillet', 'tasty', 'delicious', 'come', '2', 'piece', 'griddle', 'bread', 'amaze', 'absolutely', 'make', 'meal', 'complete', 'best', 'toast', 'ive', 'ever', 'anyway', 'cant', 'wait', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "# Assume every word is a noun.\n",
    "print([lemmatizer.lemmatize(word,pos='v') for word in yelp_best_worst.loc[0,'text_no_stopwords'].split()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"text-classification\"></a>\n",
    "\n",
    "# <font color='blue'> Text classification\n",
    "    \n",
    "We'll be training a classifier to predict the number of stars of a review based on the review text. \n",
    "\n",
    "* What are the features in this case?\n",
    "\n",
    "* What is the response variable?\n",
    "\n",
    "**Text classification is the task of predicting which category or topic a text sample is from.**\n",
    "\n",
    "We may want to identify:\n",
    "- Is an article a sports or business story?\n",
    "- Does an email have positive or negative sentiment?\n",
    "- Is the rating of a recipe 1, 2, 3, 4, or 5 stars?\n",
    "\n",
    "**Predictions are often made by using the words as features and the label as the target output.**\n",
    "\n",
    "Starting out, we will make each unique word (across all documents) a single feature. In any given corpora, we may have hundreds of thousands of unique words, so we may have hundreds of thousands of features!\n",
    "\n",
    "- For a given document, the numeric value of each feature could be the number of times the word appears in the document.\n",
    "    - So, most features will have a value of zero, resulting in a sparse matrix of features.\n",
    "\n",
    "- This technique for vectorizing text is referred to as a bag-of-words model. \n",
    "    - It is called bag of words because the document's structure is lost — as if the words are all jumbled up in a bag.\n",
    "    - The first step to creating a bag-of-words model is to create a vocabulary of all possible words in the corpora.\n",
    "\n",
    "> Alternatively, we could make each column an indicator column, which is 1 if the word is present in the document (no matter how many times) and 0 if not. This vectorization could be used to reduce the importance of repeated words. For example, a website search engine would be susceptible to spammers who load websites with repeated words. So, the search engine might use indicator columns as features rather than word counts.\n",
    "\n",
    "**We need to consider several things to decide if bag-of-words is appropriate.**\n",
    "\n",
    "- Does order of words matter?\n",
    "- Does punctuation matter?\n",
    "- Does upper or lower case matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing sets\n",
    "\n",
    "We start by splitting our dataset into a training set and a testing set.\n",
    "\n",
    "We'll train our classifier on the training set, then test its performance on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our features and response\n",
    "X = yelp_best_worst['text_no_stopwords']\n",
    "y = yelp_best_worst['stars']\n",
    "\n",
    "# Split the new DataFrame into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,test_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7473    unexpected gem wine drinker fabulous flights g...\n",
       "9598    finally teriyaki bowl place right nothing samu...\n",
       "5778    sunflower market way cheaper whole foods lot s...\n",
       "3915    yo favorite ajs gots beef prices go somewhere ...\n",
       "2713    taylors refreshing cafe menu ive breakfast far...\n",
       "                              ...                        \n",
       "2737    let tell first crush phoenix happened least ex...\n",
       "3142    dear love went museum romantic date lovely tim...\n",
       "2065    like hotdogs motor thats says get mommys oldsm...\n",
       "8596    nice facilities nice ac two fatal flaws 1 chil...\n",
       "7787    single best ribs ive ever restaurant place bra...\n",
       "Name: text_no_stopwords, Length: 1225, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7473    5\n",
       "9598    5\n",
       "5778    5\n",
       "3915    5\n",
       "2713    5\n",
       "       ..\n",
       "2737    5\n",
       "3142    5\n",
       "2065    5\n",
       "8596    1\n",
       "7787    5\n",
       "Name: stars, Length: 1225, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"count-vectoriser\"></a>\n",
    "\n",
    "# <font color='blue'> Converting documents into numerical features\n",
    "\n",
    "To use a machine learning model, we must convert unstructured text into numeric features. There are several different methods for doing this. \n",
    "\n",
    "`CountVectorizer` does what it sounds like! It converts each document into a vector of counts of different words. \n",
    "\n",
    "The result of running `CountVectorizer` across a corpus is a matrix, where each row corresponds to a document and each column corresponds to a unique words that occurs across all documents. \n",
    "\n",
    "![DTM](images/DTM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this easily using `nltk`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CountVectorizer to create document-term matrices from X_train and X_test.\n",
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1225, 11513)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rows are documents, columns are terms (aka \"tokens\" or \"features\", individual words in this situation).\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2861, 11513)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the features in our matrix easily. Let's preview a slice of our vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unexpected': 10774,\n",
       " 'gem': 4472,\n",
       " 'wine': 11275,\n",
       " 'drinker': 3399,\n",
       " 'fabulous': 3875,\n",
       " 'flights': 4149,\n",
       " 'great': 4683,\n",
       " 'selection': 9005,\n",
       " 'cant': 1783,\n",
       " 'say': 8883,\n",
       " 'much': 6786,\n",
       " 'awesome': 955,\n",
       " 'finally': 4059,\n",
       " 'teriyaki': 10277,\n",
       " 'bowl': 1459,\n",
       " 'place': 7741,\n",
       " 'right': 8617,\n",
       " 'nothing': 7046,\n",
       " 'samuri': 8821,\n",
       " 'sams': 8820,\n",
       " 'yogis': 11445,\n",
       " 'bit': 1290,\n",
       " 'better': 1229,\n",
       " 'opinion': 7217,\n",
       " 'menu': 6509,\n",
       " 'strait': 9820,\n",
       " 'forward': 4264,\n",
       " 'clearly': 2212,\n",
       " 'lists': 6106,\n",
       " 'options': 7228,\n",
       " 'available': 934,\n",
       " 'clean': 2200,\n",
       " 'staff': 9680,\n",
       " 'fast': 3944,\n",
       " 'friendly': 4338,\n",
       " 'food': 4208,\n",
       " 'also': 555,\n",
       " 'reasonably': 8309,\n",
       " 'priced': 7988,\n",
       " 'impressed': 5330,\n",
       " 'different': 3139,\n",
       " 'bowlplatesalad': 1461,\n",
       " 'able': 304,\n",
       " 'order': 7236,\n",
       " 'veggies': 10924,\n",
       " 'chicken': 2056,\n",
       " 'andor': 613,\n",
       " 'beef': 1159,\n",
       " 'pretty': 7980,\n",
       " 'however': 5191,\n",
       " 'want': 11076,\n",
       " 'extra': 3857,\n",
       " 'meat': 6449,\n",
       " 'lettuce': 6014,\n",
       " 'brown': 1565,\n",
       " 'rice': 8597,\n",
       " 'white': 11229,\n",
       " 'etc': 3719,\n",
       " 'anyway': 669,\n",
       " 'visit': 10998,\n",
       " 'like': 6055,\n",
       " 'desires': 3086,\n",
       " 'simple': 9251,\n",
       " 'decent': 2946,\n",
       " 'portion': 7869,\n",
       " 'notoverlysauced': 7056,\n",
       " 'bunch': 1634,\n",
       " 'thats': 10313,\n",
       " 'delivers': 3019,\n",
       " 'sunflower': 9966,\n",
       " 'market': 6373,\n",
       " 'way': 11135,\n",
       " 'cheaper': 2008,\n",
       " 'whole': 11236,\n",
       " 'foods': 4217,\n",
       " 'lot': 6185,\n",
       " 'sprouts': 9657,\n",
       " 'first': 4091,\n",
       " 'choice': 2111,\n",
       " 'healthy': 4942,\n",
       " 'grocery': 4727,\n",
       " 'stores': 9807,\n",
       " 'freshest': 4323,\n",
       " 'produce': 8038,\n",
       " 'around': 776,\n",
       " 'without': 11309,\n",
       " 'full': 4385,\n",
       " 'bugs': 1615,\n",
       " 'trader': 10557,\n",
       " 'joes': 5675,\n",
       " 'wide': 11247,\n",
       " 'employees': 3607,\n",
       " 'really': 8303,\n",
       " 'nice': 6932,\n",
       " 'seriously': 9040,\n",
       " 'stand': 9697,\n",
       " 'theyre': 10344,\n",
       " 'pretentious': 7978,\n",
       " 'jerks': 5654,\n",
       " 'sometimes': 9480,\n",
       " 'whether': 11218,\n",
       " 'need': 6888,\n",
       " 'lamb': 5883,\n",
       " 'bison': 1287,\n",
       " 'coconut': 2281,\n",
       " 'milk': 6588,\n",
       " 'ice': 5266,\n",
       " 'cream': 2698,\n",
       " 'almondsthis': 542,\n",
       " 'always': 561,\n",
       " 'town': 10548,\n",
       " 'every': 3749,\n",
       " 'kind': 5792,\n",
       " 'vegan': 10915,\n",
       " 'sliced': 9344,\n",
       " 'cheese': 2029,\n",
       " 'flavors': 4136,\n",
       " 'sporadic': 9632,\n",
       " 'random': 8246,\n",
       " 'never': 6919,\n",
       " 'stray': 9829,\n",
       " 'yo': 11442,\n",
       " 'favorite': 3964,\n",
       " 'ajs': 497,\n",
       " 'gots': 4630,\n",
       " 'prices': 7993,\n",
       " 'go': 4573,\n",
       " 'somewhere': 9482,\n",
       " 'else': 3584,\n",
       " 'actually': 373,\n",
       " 'arent': 757,\n",
       " 'higher': 5025,\n",
       " 'safeway': 8769,\n",
       " 'carry': 1841,\n",
       " 'expensive': 3819,\n",
       " 'buy': 1686,\n",
       " 'products': 8041,\n",
       " 'far': 3931,\n",
       " 'ordering': 7238,\n",
       " 'rambutans': 8239,\n",
       " 'whim': 11220,\n",
       " 'craving': 2690,\n",
       " 'sandwich': 8832,\n",
       " 'counter': 2613,\n",
       " 'love': 6199,\n",
       " 'slices': 9345,\n",
       " 'pizza': 7733,\n",
       " 'big': 1255,\n",
       " 'head': 4929,\n",
       " 'drink': 3398,\n",
       " 'stellar': 9756,\n",
       " 'brew': 1509,\n",
       " 'china': 2087,\n",
       " 'mist': 6662,\n",
       " 'iced': 5270,\n",
       " 'teas': 10211,\n",
       " 'prickly': 8000,\n",
       " 'pear': 7540,\n",
       " 'flavor': 4129,\n",
       " 'comes': 2329,\n",
       " 'im': 5301,\n",
       " 'happy': 4876,\n",
       " 'store': 9806,\n",
       " 'larger': 5900,\n",
       " 'scottsdalelincoln': 8929,\n",
       " 'location': 6138,\n",
       " 'service': 9049,\n",
       " 'friendlier': 4335,\n",
       " 'well': 11180,\n",
       " 'shot': 9176,\n",
       " 'gerri': 4494,\n",
       " 'floral': 4162,\n",
       " 'teriffic': 10276,\n",
       " 'manager': 6320,\n",
       " 'takes': 10119,\n",
       " 'care': 1815,\n",
       " 'time': 10423,\n",
       " 'guys': 4794,\n",
       " 'rock': 8660,\n",
       " 'taylors': 10197,\n",
       " 'refreshing': 8385,\n",
       " 'cafe': 1709,\n",
       " 'ive': 5599,\n",
       " 'breakfast': 1494,\n",
       " 'often': 7136,\n",
       " 'lunch': 6236,\n",
       " 'hope': 5131,\n",
       " 'try': 10648,\n",
       " 'dinner': 3168,\n",
       " 'soon': 9494,\n",
       " 'bc': 1126,\n",
       " 'think': 10357,\n",
       " 'feature': 3977,\n",
       " 'live': 6113,\n",
       " 'jazz': 5636,\n",
       " 'wednesday': 11156,\n",
       " 'nights': 6951,\n",
       " 'ever': 3748,\n",
       " 'bad': 992,\n",
       " 'meal': 6435,\n",
       " 'fact': 3885,\n",
       " 'hard': 4880,\n",
       " 'deciding': 2951,\n",
       " 'highlights': 5029,\n",
       " 'huevos': 5215,\n",
       " 'rancheros': 8244,\n",
       " 'eggs': 3552,\n",
       " 'benedict': 1207,\n",
       " 'waffles': 11037,\n",
       " 'mexican': 6547,\n",
       " 'omelet': 7167,\n",
       " 'liked': 6057,\n",
       " 'bloody': 1345,\n",
       " 'marys': 6390,\n",
       " 'one': 7176,\n",
       " 'tried': 10617,\n",
       " 'friends': 4339,\n",
       " 'hooked': 5123,\n",
       " 'since': 9259,\n",
       " 'less': 6006,\n",
       " 'intoxicating': 5519,\n",
       " 'beverage': 1232,\n",
       " 'chai': 1946,\n",
       " 'latte': 5917,\n",
       " 'careful': 1819,\n",
       " 'good': 4604,\n",
       " 'youll': 11456,\n",
       " 'another': 638,\n",
       " 'know': 5823,\n",
       " 'bon': 1392,\n",
       " 'appetite': 701,\n",
       " 'get': 4496,\n",
       " 'quality': 8175,\n",
       " 'control': 2520,\n",
       " 'wont': 11329,\n",
       " 'eat': 3499,\n",
       " 'except': 3778,\n",
       " 'ate': 873,\n",
       " 'last': 5907,\n",
       " 'weekend': 11161,\n",
       " 'roasted': 8654,\n",
       " 'problem': 8027,\n",
       " 'tasted': 10171,\n",
       " 'basted': 1105,\n",
       " 'cinnamon': 2163,\n",
       " 'sauce': 8865,\n",
       " 'risotto': 8636,\n",
       " 'least': 5962,\n",
       " '10': 1,\n",
       " 'peppercorns': 7580,\n",
       " 'plates': 7770,\n",
       " 'realized': 8300,\n",
       " 'crawling': 2693,\n",
       " 'couldve': 2610,\n",
       " 'strained': 9819,\n",
       " 'processed': 8034,\n",
       " 'whatever': 11204,\n",
       " 'dont': 3315,\n",
       " 'serve': 9042,\n",
       " 'plate': 7769,\n",
       " 'previous': 7984,\n",
       " 'ordered': 7237,\n",
       " 'exact': 3766,\n",
       " 'dish': 3220,\n",
       " 'let': 6009,\n",
       " 'management': 6319,\n",
       " 'unacceptable': 10735,\n",
       " 'said': 8774,\n",
       " 'best': 1224,\n",
       " 'could': 2608,\n",
       " '20': 92,\n",
       " 'discount': 3201,\n",
       " 'werent': 11192,\n",
       " 'looking': 6166,\n",
       " 'free': 4301,\n",
       " 'discounted': 3202,\n",
       " 'rather': 8264,\n",
       " 'acknowledgement': 355,\n",
       " 'cared': 1816,\n",
       " 'happened': 4870,\n",
       " 'unless': 10798,\n",
       " 'something': 9476,\n",
       " 'changes': 1974,\n",
       " 'burgers': 1640,\n",
       " 'probably': 8025,\n",
       " 'worthy': 11370,\n",
       " 'four': 4270,\n",
       " 'star': 9706,\n",
       " 'rating': 8265,\n",
       " 'especially': 3705,\n",
       " 'considering': 2475,\n",
       " 'difficult': 3141,\n",
       " 'find': 4064,\n",
       " 'even': 3742,\n",
       " 'passable': 7466,\n",
       " 'italian': 5577,\n",
       " 'west': 11194,\n",
       " 'valley': 10889,\n",
       " 'bumped': 1629,\n",
       " 'grazie': 4678,\n",
       " 'fourstars': 4271,\n",
       " 'fivestars': 4102,\n",
       " 'ownership': 7339,\n",
       " 'attentive': 900,\n",
       " 'atmosphere': 881,\n",
       " 'pleasant': 7792,\n",
       " 'particular': 7456,\n",
       " 'ny': 7085,\n",
       " 'native': 6860,\n",
       " 'finding': 4065,\n",
       " 'phoenix': 7648,\n",
       " 'close': 2234,\n",
       " 'impossible': 5327,\n",
       " 'thincrust': 10350,\n",
       " 'italianstyle': 5579,\n",
       " 'opposed': 7221,\n",
       " 'chicagostyle': 2055,\n",
       " 'happen': 4869,\n",
       " 'easier': 3490,\n",
       " 'execute': 3793,\n",
       " 'would': 11373,\n",
       " 'loved': 6200,\n",
       " 'slice': 9343,\n",
       " 'instead': 5471,\n",
       " 'sure': 10005,\n",
       " 'nonetheless': 6994,\n",
       " 'awful': 959,\n",
       " 'waited': 11043,\n",
       " 'patiently': 7495,\n",
       " '15': 62,\n",
       " 'minutes': 6632,\n",
       " 'server': 9044,\n",
       " 'come': 2327,\n",
       " 'table': 10085,\n",
       " 'hostess': 5159,\n",
       " 'came': 1748,\n",
       " 'noticed': 7051,\n",
       " 'nobody': 6972,\n",
       " 'helping': 4988,\n",
       " 'us': 10857,\n",
       " 'got': 4628,\n",
       " 'drinks': 3401,\n",
       " 'roll': 8671,\n",
       " 'decides': 2950,\n",
       " 'make': 6292,\n",
       " 'appearance': 696,\n",
       " 'saying': 8886,\n",
       " 'decided': 2948,\n",
       " 'yet': 11434,\n",
       " 'uhh': 10722,\n",
       " 'yeah': 11412,\n",
       " 'overall': 7299,\n",
       " 'poor': 7851,\n",
       " 'okay': 7151,\n",
       " 'going': 4589,\n",
       " 'back': 972,\n",
       " 'tired': 10445,\n",
       " 'smash': 9382,\n",
       " 'burger': 1639,\n",
       " 'woohoo': 11339,\n",
       " 'crispy': 2739,\n",
       " 'thick': 10346,\n",
       " 'perfectly': 7591,\n",
       " 'build': 1616,\n",
       " 'egg': 3547,\n",
       " 'bun': 1632,\n",
       " 'mayo': 6425,\n",
       " 'swiss': 10063,\n",
       " 'hello': 4980,\n",
       " 'mama': 6312,\n",
       " 'fries': 4342,\n",
       " 'went': 11191,\n",
       " 'boyfriend': 1470,\n",
       " 'afternoon': 448,\n",
       " 'green': 4691,\n",
       " 'apple': 708,\n",
       " 'snow': 9434,\n",
       " 'mocha': 6682,\n",
       " 'blast': 1315,\n",
       " 'amazing': 570,\n",
       " 'boba': 1371,\n",
       " 'squishy': 9670,\n",
       " 'perfect': 7587,\n",
       " 'taste': 10170,\n",
       " 'shop': 9158,\n",
       " 'cool': 2541,\n",
       " 'battleship': 1121,\n",
       " 'checkers': 2015,\n",
       " 'board': 1365,\n",
       " 'games': 4431,\n",
       " 'customers': 2835,\n",
       " 'play': 7776,\n",
       " 'computer': 2411,\n",
       " 'monitor': 6715,\n",
       " 'guessing': 4770,\n",
       " 'didnt': 3130,\n",
       " 'look': 6164,\n",
       " 'music': 6818,\n",
       " 'remember': 8445,\n",
       " 'ones': 7180,\n",
       " 'use': 10859,\n",
       " 'debitcredit': 2937,\n",
       " 'card': 1808,\n",
       " 'friend': 4334,\n",
       " 'asked': 832,\n",
       " 'ready': 8295,\n",
       " 'leave': 5966,\n",
       " 'thing': 10351,\n",
       " 'update': 10832,\n",
       " 'number': 7069,\n",
       " 'god': 4581,\n",
       " 'bless': 1326,\n",
       " 'lifetime': 6042,\n",
       " 'fitness': 4097,\n",
       " 'tell': 10239,\n",
       " 'despise': 3091,\n",
       " 'working': 11350,\n",
       " 'times': 10429,\n",
       " 'id': 5275,\n",
       " 'pee': 7554,\n",
       " 'within': 11308,\n",
       " '50': 203,\n",
       " 'feet': 3993,\n",
       " 'stair': 9688,\n",
       " 'stepper': 9763,\n",
       " 'ltf': 6218,\n",
       " 'abbreviate': 300,\n",
       " 'used': 10860,\n",
       " 'makes': 6294,\n",
       " 'gym': 4796,\n",
       " 'experience': 3820,\n",
       " 'fairly': 3896,\n",
       " 'painless': 7371,\n",
       " 'steve': 9769,\n",
       " 'salute': 8810,\n",
       " 'aknowledging': 500,\n",
       " 'value': 10893,\n",
       " 'fine': 4068,\n",
       " 'workout': 11351,\n",
       " 'establishment': 3712,\n",
       " 'many': 6344,\n",
       " 'opinions': 7218,\n",
       " 'hear': 4945,\n",
       " 'talk': 10128,\n",
       " 'ill': 5294,\n",
       " 'reiterate': 8419,\n",
       " 'points': 7822,\n",
       " 'absolutely': 309,\n",
       " 'fan': 3922,\n",
       " 'children': 2071,\n",
       " 'childcare': 2068,\n",
       " 'kiddos': 5779,\n",
       " '12': 37,\n",
       " 'singles': 9270,\n",
       " 'flirt': 4153,\n",
       " 'kids': 5781,\n",
       " 'sniffling': 9427,\n",
       " 'screaming': 8941,\n",
       " 'swim': 10057,\n",
       " 'outside': 7293,\n",
       " 'parental': 7435,\n",
       " 'supervision': 9986,\n",
       " 'pool': 7848,\n",
       " 'complete': 2391,\n",
       " 'waterslides': 11128,\n",
       " 'kidspecifc': 5782,\n",
       " 'water': 11120,\n",
       " 'fountain': 4268,\n",
       " 'spray': 9648,\n",
       " 'mean': 6439,\n",
       " 'loveitloveit': 6201,\n",
       " 'lifetimes': 6043,\n",
       " 'members': 6491,\n",
       " 'span': 9554,\n",
       " 'shape': 9098,\n",
       " 'person': 7607,\n",
       " 'scenery': 8905,\n",
       " 'watch': 11113,\n",
       " 'inspire': 5464,\n",
       " 'keep': 5750,\n",
       " 'forsaken': 4258,\n",
       " 'may': 6423,\n",
       " 'see': 8989,\n",
       " 'professional': 8043,\n",
       " 'athelete': 875,\n",
       " 'tv': 10683,\n",
       " 'personality': 7610,\n",
       " 'occasion': 7104,\n",
       " 'doesnt': 3289,\n",
       " 'wait': 11042,\n",
       " 'piece': 7687,\n",
       " 'equipement': 3689,\n",
       " 'tons': 10495,\n",
       " 'machines': 6255,\n",
       " 'covered': 2640,\n",
       " 'meathead': 6454,\n",
       " 'sweat': 10039,\n",
       " 'part': 7453,\n",
       " 'group': 4737,\n",
       " 'classes': 2194,\n",
       " 'excellent': 3776,\n",
       " 'teachers': 10201,\n",
       " 'day': 2905,\n",
       " 'week': 11159,\n",
       " 'variety': 10904,\n",
       " 'coreographed': 2563,\n",
       " 'hiphop': 5049,\n",
       " 'kickboxing': 5772,\n",
       " 'jam': 5616,\n",
       " 'damn': 2867,\n",
       " 'locker': 6142,\n",
       " 'rooms': 8689,\n",
       " 'spotless': 9640,\n",
       " 'towel': 10543,\n",
       " 'sauna': 8871,\n",
       " 'steam': 9747,\n",
       " 'weirdo': 11174,\n",
       " 'naked': 6843,\n",
       " 'people': 7572,\n",
       " 'device': 3112,\n",
       " 'dries': 3394,\n",
       " 'bathing': 1112,\n",
       " 'suit': 9942,\n",
       " 'minute': 6631,\n",
       " 'ingenious': 5422,\n",
       " 'member': 6489,\n",
       " 'ton': 10485,\n",
       " 'scottsdale': 8928,\n",
       " 'gyms': 4797,\n",
       " 'drive': 3402,\n",
       " 'rush': 8750,\n",
       " 'hour': 5174,\n",
       " 'tempe': 10244,\n",
       " 'road': 8648,\n",
       " 'rage': 8223,\n",
       " 'burn': 1643,\n",
       " 'calories': 1744,\n",
       " 'winwin': 11287,\n",
       " 'situation': 9294,\n",
       " 'bar': 1050,\n",
       " 'none': 6993,\n",
       " 'salted': 8805,\n",
       " 'caramel': 1805,\n",
       " 'wonderful': 11321,\n",
       " 'felt': 3996,\n",
       " 'years': 11416,\n",
       " 'old': 7156,\n",
       " 'enjoying': 3643,\n",
       " 'cone': 2441,\n",
       " 'decor': 2959,\n",
       " 'braille': 1474,\n",
       " 'wall': 11062,\n",
       " 'although': 559,\n",
       " 'wonder': 11319,\n",
       " 'exactly': 3767,\n",
       " 'says': 8888,\n",
       " 'bought': 1445,\n",
       " 'pint': 7715,\n",
       " 'sweet': 10044,\n",
       " 'republic': 8496,\n",
       " 'everything': 3756,\n",
       " 'stands': 9702,\n",
       " 'totally': 10529,\n",
       " 'recommended': 8342,\n",
       " 'called': 1735,\n",
       " 'wanted': 11077,\n",
       " 'charge': 1986,\n",
       " '30': 142,\n",
       " 'fingerprints': 4075,\n",
       " 'joke': 5685,\n",
       " 'found': 4265,\n",
       " 'side': 9217,\n",
       " 'air': 485,\n",
       " 'park': 7437,\n",
       " 'caveat': 1893,\n",
       " 'emptor': 3611,\n",
       " 'give': 4529,\n",
       " 'zero': 11491,\n",
       " 'airpark': 490,\n",
       " 'greenwayhayden': 4695,\n",
       " 'loop': 6171,\n",
       " 'global': 4554,\n",
       " 'pi': 7666,\n",
       " 'security': 8983,\n",
       " 'prints': 8013,\n",
       " 'address': 391,\n",
       " '7807': 256,\n",
       " 'east': 3492,\n",
       " 'greenway': 4694,\n",
       " 'suite': 9944,\n",
       " 'az': 964,\n",
       " 'globalpisecurity': 4555,\n",
       " 'dot': 3333,\n",
       " 'com': 2318,\n",
       " 'luck': 6224,\n",
       " 'chinese': 2089,\n",
       " 'others': 7267,\n",
       " 'salad': 8784,\n",
       " 'dressing': 3390,\n",
       " 'addictive': 385,\n",
       " 'take': 10114,\n",
       " 'home': 5099,\n",
       " 'containers': 2499,\n",
       " 'shrimp': 9200,\n",
       " 'dishes': 3221,\n",
       " 'outstanding': 7296,\n",
       " 'large': 5898,\n",
       " 'plump': 7806,\n",
       " 'high': 5021,\n",
       " 'pieces': 7688,\n",
       " 'average': 938,\n",
       " 'enjoy': 3640,\n",
       " 'true': 10639,\n",
       " 'took': 10497,\n",
       " 'dog': 3290,\n",
       " 'bouncing': 1453,\n",
       " 'trying': 10650,\n",
       " 'groomer': 4729,\n",
       " 'read': 8292,\n",
       " 'reviews': 8582,\n",
       " 'gave': 4461,\n",
       " 'customer': 2834,\n",
       " 'performed': 7594,\n",
       " 'appt': 738,\n",
       " 'importantlymy': 5324,\n",
       " 'seem': 8995,\n",
       " 'anxious': 661,\n",
       " 'uncomfortable': 10748,\n",
       " 'normally': 7026,\n",
       " 'leaves': 5968,\n",
       " 'stressed': 9835,\n",
       " 'seemed': 8996,\n",
       " 'ease': 3489,\n",
       " 'stephen': 9761,\n",
       " 'hit': 5061,\n",
       " 'nail': 6838,\n",
       " 'peaceful': 7532,\n",
       " 'resortspa': 8529,\n",
       " 'advantage': 421,\n",
       " 'horseback': 5148,\n",
       " 'riding': 8615,\n",
       " 'resort': 8526,\n",
       " 'definitely': 2986,\n",
       " 'exceeded': 3773,\n",
       " 'expectations': 3812,\n",
       " 'accommodating': 334,\n",
       " 'delicious': 3005,\n",
       " 'beautiful': 1144,\n",
       " 'wellmaintained': 11185,\n",
       " 'everyone': 3753,\n",
       " 'encountered': 3618,\n",
       " 'mile': 6586,\n",
       " 'regard': 8397,\n",
       " 'nyc': 7086,\n",
       " 'completely': 2393,\n",
       " 'rejuvenated': 8421,\n",
       " 'wish': 11301,\n",
       " 'lived': 6114,\n",
       " 'stopped': 9802,\n",
       " 'yogurtland': 11447,\n",
       " 'spring': 9650,\n",
       " 'training': 10569,\n",
       " 'stayed': 9736,\n",
       " 'forever': 4237,\n",
       " 'frozen': 4357,\n",
       " 'yogurt': 11446,\n",
       " 'impeccable': 5315,\n",
       " 'mixed': 6668,\n",
       " 'raspberry': 8260,\n",
       " 'cheesecake': 2033,\n",
       " 'vanilla': 10897,\n",
       " 'bean': 1130,\n",
       " 'pbj': 7528,\n",
       " 'um': 10729,\n",
       " 'yes': 11431,\n",
       " 'tangerine': 10144,\n",
       " 'tango': 10146,\n",
       " 'usual': 10867,\n",
       " 'goto': 4629,\n",
       " 'tart': 10163,\n",
       " 'extensive': 3854,\n",
       " 'toppings': 10511,\n",
       " 'price': 7987,\n",
       " 'froyo': 4356,\n",
       " 'next': 6930,\n",
       " 'places': 7746,\n",
       " '5th': 220,\n",
       " 'recommend': 8339,\n",
       " 'anyone': 665,\n",
       " 'asks': 834,\n",
       " 'appetizers': 705,\n",
       " 'specials': 9577,\n",
       " 'lets': 6010,\n",
       " 'start': 9713,\n",
       " 'glass': 4541,\n",
       " 'list': 6099,\n",
       " 'pm': 7810,\n",
       " 'someone': 9474,\n",
       " 'mention': 6505,\n",
       " 'yep': 11428,\n",
       " 'fried': 4333,\n",
       " 'pickles': 7675,\n",
       " 'admit': 405,\n",
       " 'convincing': 2533,\n",
       " 'win': 11267,\n",
       " 'thanks': 10308,\n",
       " 'scott': 8927,\n",
       " 'thinly': 10360,\n",
       " 'cut': 2837,\n",
       " 'perfection': 7589,\n",
       " 'served': 9043,\n",
       " 'creamy': 2704,\n",
       " 'horseradish': 5149,\n",
       " 'dipping': 3176,\n",
       " 'known': 5829,\n",
       " 'bruchetta': 1575,\n",
       " 'types': 10709,\n",
       " 'favorites': 3965,\n",
       " 'brie': 1523,\n",
       " 'tomato': 10482,\n",
       " 'basil': 1096,\n",
       " 'mozzarella': 6776,\n",
       " 'steak': 9740,\n",
       " 'threesome': 10381,\n",
       " 'mentioned': 6506,\n",
       " 'night': 6945,\n",
       " 'bread': 1488,\n",
       " 'flavorful': 4131,\n",
       " 'toasted': 10455,\n",
       " 'entire': 3666,\n",
       " 'review': 8577,\n",
       " 'httpschmoozeazcomblogp680': 5194,\n",
       " 'tip': 10437,\n",
       " 'mesagateway': 6523,\n",
       " 'fliers': 4146,\n",
       " 'airport': 492,\n",
       " 'itlove': 5591,\n",
       " 'made': 6262,\n",
       " 'lowcarb': 6211,\n",
       " 'gurl': 4785,\n",
       " 'wow': 11376,\n",
       " 'enjoyed': 3642,\n",
       " 'meals': 6437,\n",
       " 'hands': 4854,\n",
       " 'chef': 2042,\n",
       " 'gross': 4732,\n",
       " 'past': 7474,\n",
       " 'blown': 1354,\n",
       " 'away': 952,\n",
       " 'planning': 7759,\n",
       " 'garden': 4443,\n",
       " 'party': 7462,\n",
       " 'bridal': 1521,\n",
       " 'shower': 9190,\n",
       " 'record': 8347,\n",
       " 'temperatures': 10248,\n",
       " 'left': 5975,\n",
       " 'ravaged': 8271,\n",
       " 'heat': 4955,\n",
       " 'sweltering': 10054,\n",
       " 'ladies': 5871,\n",
       " 'unable': 10734,\n",
       " 'small': 9373,\n",
       " 'response': 8535,\n",
       " 'attendance': 893,\n",
       " 'dining': 3166,\n",
       " 'spots': 9643,\n",
       " 'offer': 7127,\n",
       " 'budget': 1606,\n",
       " 'classy': 2199,\n",
       " 'christophers': 2138,\n",
       " 'truly': 10643,\n",
       " 'rose': 8698,\n",
       " 'alex': 513,\n",
       " 'special': 9572,\n",
       " 'event': 3745,\n",
       " 'contact': 2494,\n",
       " 'lady': 5873,\n",
       " 'emails': 3590,\n",
       " 'phone': 7651,\n",
       " 'calls': 1739,\n",
       " 'returned': 8567,\n",
       " 'lightning': 6053,\n",
       " 'speed': 9589,\n",
       " 'presented': 7962,\n",
       " 'delightful': 3011,\n",
       " 'plenty': 7800,\n",
       " 'choices': 2112,\n",
       " 'vegetarians': 10920,\n",
       " 'attending': 898,\n",
       " 'range': 8250,\n",
       " '18': 75,\n",
       " 'private': 8019,\n",
       " 'room': 8687,\n",
       " 'fit': 4095,\n",
       " 'festivities': 4008,\n",
       " 'sundays': 9961,\n",
       " 'quiet': 8200,\n",
       " 'biltmore': 1274,\n",
       " 'fashion': 3942,\n",
       " 'lended': 5995,\n",
       " 'intimacy': 5511,\n",
       " 'ben': 1201,\n",
       " 'guy': 4793,\n",
       " 'needed': 6889,\n",
       " 'helpers': 4985,\n",
       " 'polite': 7834,\n",
       " 'chris': 2132,\n",
       " 'vegetable': 10917,\n",
       " 'ans': 639,\n",
       " 'cooked': 2535,\n",
       " 'kitchen': 5806,\n",
       " 'quickly': 8199,\n",
       " 'together': 10470,\n",
       " 'feel': 3988,\n",
       " 'rushed': 8751,\n",
       " 'flowed': 4172,\n",
       " 'desserts': 3096,\n",
       " 'gobbled': 4579,\n",
       " 'oohed': 7198,\n",
       " 'awed': 954,\n",
       " 'offered': 7128,\n",
       " 'gifts': 4507,\n",
       " 'car': 1803,\n",
       " 'bride': 1522,\n",
       " 'saving': 8877,\n",
       " 'team': 10205,\n",
       " 'telling': 10241,\n",
       " 'job': 5671,\n",
       " 'holidays': 5093,\n",
       " 'coming': 2337,\n",
       " 'reminded': 8450,\n",
       " 'catering': 1882,\n",
       " 'hmmm': 5069,\n",
       " 'christmas': 2135,\n",
       " '1st': 91,\n",
       " 'closed': 2235,\n",
       " 'hours': 5178,\n",
       " 'supposed': 9997,\n",
       " 'open': 7202,\n",
       " 'calling': 1737,\n",
       " 'normal': 7024,\n",
       " 'business': 1660,\n",
       " 'pickup': 7677,\n",
       " 'family': 3914,\n",
       " 'owned': 7334,\n",
       " 'weve': 11199,\n",
       " 'consistently': 2480,\n",
       " 'fresh': 4320,\n",
       " 'homemake': 5104,\n",
       " 'things': 10353,\n",
       " 'ranch': 8243,\n",
       " 'level': 6016,\n",
       " 'highly': 5030,\n",
       " 'either': 3559,\n",
       " 'hot': 5162,\n",
       " 'summer': 9952,\n",
       " 'paletas': 7388,\n",
       " 'ingredients': 5426,\n",
       " 'welcoming': 11179,\n",
       " 'environment': 3678,\n",
       " 'downtown': 3353,\n",
       " 'chandler': 1971,\n",
       " 'near': 6871,\n",
       " 'meats': 6457,\n",
       " 'note': 7042,\n",
       " 'help': 4983,\n",
       " 'works': 11353,\n",
       " 'seems': 8997,\n",
       " 'issue': 5574,\n",
       " 'couple': 2622,\n",
       " 'restaurant': 8544,\n",
       " 'fancy': 3925,\n",
       " 'eaten': 3500,\n",
       " 'frequently': 4317,\n",
       " 'takeout': 10118,\n",
       " 'gobi': 4580,\n",
       " 'manchurian': 6326,\n",
       " 'vindaloo': 10982,\n",
       " 'curry': 2820,\n",
       " 'buffet': 1611,\n",
       " 'several': 9071,\n",
       " 'turkey': 10674,\n",
       " 'provolone': 8097,\n",
       " 'cold': 2296,\n",
       " 'sub': 9899,\n",
       " 'added': 379,\n",
       " 'simply': 9255,\n",
       " 'hubby': 5207,\n",
       " 'nypd': 7087,\n",
       " 'door': 3320,\n",
       " 'thought': 10372,\n",
       " 'new': 6920,\n",
       " 'sit': 9286,\n",
       " 'grab': 4639,\n",
       " 'rotating': 8704,\n",
       " 'decide': 2947,\n",
       " 'seeing': 8992,\n",
       " 'king': 5798,\n",
       " 'queen': 8182,\n",
       " '2000': 94,\n",
       " 'pricing': 7999,\n",
       " 'affordable': 441,\n",
       " 'date': 2892,\n",
       " 'year': 11413,\n",
       " 'girlfriends': 4524,\n",
       " 'sleek': 9337,\n",
       " 'cozy': 2654,\n",
       " 'hopping': 5137,\n",
       " 'overly': 7316,\n",
       " 'crowded': 2752,\n",
       " 'welcome': 11176,\n",
       " 'sight': 9228,\n",
       " 'spent': 9597,\n",
       " 'hotspots': 5172,\n",
       " 'older': 7157,\n",
       " 'crave': 2688,\n",
       " 'intimate': 5512,\n",
       " 'setting': 9063,\n",
       " 'sidebar': 9220,\n",
       " 'fits': 4098,\n",
       " 'bill': 1269,\n",
       " 'five': 4101,\n",
       " 'stars': 9712,\n",
       " 'goes': 4586,\n",
       " 'cause': 1887,\n",
       " 'twitter': 10699,\n",
       " 'follow': 4198,\n",
       " 'buzzcation': 1690,\n",
       " 'sidebarphx': 9221,\n",
       " 'folks': 4197,\n",
       " 'hold': 5085,\n",
       " 'networking': 6914,\n",
       " 'fundraising': 4397,\n",
       " 'crew': 2729,\n",
       " 'joing': 5681,\n",
       " 'commend': 2340,\n",
       " 'accomodating': 336,\n",
       " 'crazy': 2696,\n",
       " 'tweeples': 10688,\n",
       " 'mine': 6607,\n",
       " 'friday': 4330,\n",
       " 'double': 3335,\n",
       " 'charged': 1987,\n",
       " 'bartender': 1083,\n",
       " 'threw': 10383,\n",
       " 'receipt': 8320,\n",
       " 'avoid': 943,\n",
       " 'cancelling': 1766,\n",
       " '60': 221,\n",
       " 'amount': 597,\n",
       " 'claims': 2184,\n",
       " 'assumes': 860,\n",
       " '100': 2,\n",
       " 'tips': 10442,\n",
       " 'ummyeah': 10732,\n",
       " 'skip': 9321,\n",
       " 'along': 548,\n",
       " 'grew': 4702,\n",
       " 'area': 752,\n",
       " 'dad': 2857,\n",
       " 'brother': 1561,\n",
       " 'check': 2012,\n",
       " 'yelp': 11422,\n",
       " 'addict': 381,\n",
       " 'saw': 8881,\n",
       " 'skeptical': 9311,\n",
       " 'driving': 3410,\n",
       " 'typical': 10710,\n",
       " 'hole': 5089,\n",
       " 'disappointed': 3191,\n",
       " 'carne': 1829,\n",
       " 'asada': 815,\n",
       " 'burrito': 1648,\n",
       " 'aside': 830,\n",
       " 'aguas': 471,\n",
       " 'frescas': 4319,\n",
       " 'particularly': 7457,\n",
       " 'orange': 7231,\n",
       " 'oneso': 7181,\n",
       " 'regret': 8407,\n",
       " 'single': 9269,\n",
       " 'gone': 4600,\n",
       " 'countless': 2617,\n",
       " 'bringing': 1536,\n",
       " 'convinced': 2532,\n",
       " 'anywhere': 674,\n",
       " 'blonde': 1343,\n",
       " 'gringa': 4716,\n",
       " 'craptastic': 2684,\n",
       " 'eating': 3507,\n",
       " 'sushi': 10023,\n",
       " 'across': 361,\n",
       " 'nation': 6857,\n",
       " 'scottsdales': 8930,\n",
       " 'geisha': 4467,\n",
       " 'surprised': 10010,\n",
       " 'ultimate': 10725,\n",
       " 'low': 6208,\n",
       " 'andrew': 615,\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams are features which consist of N consecutive words. This is useful because using the bag-of-words model, treating `data scientist` as a single feature has more meaning than having two independent features `data` and `scientist`!\n",
    "\n",
    "Example:\n",
    "```\n",
    "my cat is awesome\n",
    "Unigrams (1-grams): 'my', 'cat', 'is', 'awesome'\n",
    "Bigrams (2-grams): 'my cat', 'cat is', 'is awesome'\n",
    "Trigrams (3-grams): 'my cat is', 'cat is awesome'\n",
    "4-grams: 'my cat is awesome'\n",
    "```\n",
    "\n",
    "- **ngram_range:** tuple (min_n, max_n)\n",
    "- The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1225, 68327)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Include 1-grams and 2-grams.\n",
    "vect = CountVectorizer(ngram_range=(1, 2),stop_words='english')\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start to see how supplementing our features with n-grams can lead to more feature columns. When we produce n-grams from a document with $W$ words, we add an additional $(n-W+1)$ features (at most). That said, be careful — when we compute n-grams from an entire corpus, the number of _unique_ n-grams could be vastly higher than the number of _unique_ unigrams! This could cause an undesired feature explosion.\n",
    "\n",
    "Although we sometimes add important new features that have meaning such as `data scientist`, many of the new features will just be noise. So, particularly if we do not have much data, adding n-grams can actually decrease model performance. This is because if each n-gram is only present once or twice in the training set, we are effectively adding mostly noisy features to the mix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['allowed sample', 'allowed time', 'allowed work', 'allowing', 'allowing continue', 'allowing extremely', 'allowing time', 'allows', 'allows able', 'allows patrons', 'allows person', 'allows premises', 'allstar', 'allstar opened', 'allstars', 'allstars play', 'allure', 'allure trails', 'alluringmaybe', 'alluringmaybe sushi', 'almond', 'almond brittle', 'almond buttercrunch', 'almond chocolate', 'almond croissant', 'almond croissants', 'almonds', 'almonds mixed', 'almondsthis', 'almondsthis choice', 'aloe', 'aloe vera', 'alofts', 'alofts probably', 'alofts youve', 'aloha', 'aloha hour', 'aloha kitchen', 'alongside', 'alongside barely', 'alongside steak', 'alons', 'alons rock', 'aloof', 'aloof fawning', 'alot', 'alot better', 'alot cheese', 'alot cupcakes', 'alot flavor']\n"
     ]
    }
   ],
   "source": [
    "# Last 50 features\n",
    "print((vect.get_feature_names()[2000:2050]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cvec_opt'></a>\n",
    "### Other CountVectorizer Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `max_features`: int or None, default=None\n",
    "- If not None, build a vocabulary that only consider the top `max_features` ordered by term frequency across the corpus. This allows us to keep more common n-grams and remove ones that may appear once. If we include words that only occur once, this can lead to said features being highly associated with a class and cause overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove English stop words and only keep 100 features.\n",
    "vect = CountVectorizer(ngram_range=(1, 2),stop_words='english', max_features=100)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazing', 'area', 'awesome', 'bad', 'bar', 'best', 'better', 'big', 'came', 'check', 'cheese', 'chicken', 'come', 'day', 'definitely', 'delicious', 'didnt', 'different', 'dinner', 'dont', 'eat', 'excellent', 'experience', 'family', 'favorite', 'feel', 'food', 'fresh', 'friendly', 'friends', 'going', 'good', 'got', 'great', 'happy', 'home', 'hot', 'hour', 'house', 'im', 'ive', 'know', 'like', 'little', 'location', 'long', 'look', 'looking', 'love', 'lunch', 'make', 'meal', 'menu', 'minutes', 'new', 'nice', 'night', 'old', 'order', 'ordered', 'people', 'perfect', 'phoenix', 'pizza', 'place', 'pretty', 'price', 'prices', 'really', 'recommend', 'restaurant', 'review', 'right', 'room', 'said', 'salad', 'sauce', 'say', 'service', 'small', 'special', 'staff', 'store', 'sure', 'table', 'thing', 'things', 'think', 'time', 'times', 'told', 'took', 'try', 'wait', 'want', 'way', 'went', 'worth', 'years', 'youre']\n"
     ]
    }
   ],
   "source": [
    "# All 100 features\n",
    "print((vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with all other models, more features does not mean a better model. So, we must tune our feature generator to remove features whose predictive capability is none or very low.\n",
    "\n",
    "In this case, there is roughly a 1.6% increase in accuracy when we double the n-gram size and increase our max features by 1,000-fold. Note that if we restrict it to only unigrams, then the accuracy increases even more! So, bigrams were very likely adding more noise than signal. \n",
    "\n",
    "In the end, by only using 16,000 unigram features we came away with a much smaller, simpler, and easier-to-think-about model which also resulted in higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-grams and 2-grams, up to 100K features:\n",
      "['able open', 'able order', 'able pay', 'able properly', 'able purchase', 'able reference', 'able say', 'able seat', 'able seated', 'able see']\n",
      "1-grams only, up to 100K features:\n",
      "['baggage', 'bagged', 'bagginsey', 'baggy', 'bagjust', 'bags', 'baguettes', 'bahn', 'bait', 'baja']\n"
     ]
    }
   ],
   "source": [
    "# Include 1-grams and 2-grams, and limit the number of features.\n",
    "\n",
    "print('1-grams and 2-grams, up to 100K features:')\n",
    "vect = CountVectorizer(ngram_range=(1, 2), max_features=100000)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "print(vect.get_feature_names()[1000:1010])\n",
    "\n",
    "print('1-grams only, up to 100K features:')\n",
    "vect = CountVectorizer(ngram_range=(1, 1), max_features=100000)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "print(vect.get_feature_names()[1000:1010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `min_df`: Float in range [0.0, 1.0] or int, default=1\n",
    "- When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bite', 'bite good', 'bite like', 'bites', 'bitterness', 'black', 'black bean', 'black beans', 'black white', 'blah']\n"
     ]
    }
   ],
   "source": [
    "# Include 1-grams and 2-grams, and only include terms that appear at least two times.\n",
    "vect = CountVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "print(vect.get_feature_names()[1000:1010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bayes\"></a>\n",
    "\n",
    "# <font color='blue'> Text Classification with Naive Bayes\n",
    "\n",
    "Naive Bayes is a very popular classifier because it has minimal storage requirements, is fast, can be tuned easily with more data, and has found very useful applications in text classificaton. For example, Paul Graham originally proposed using Naive Bayes to detect spam in his [Plan for Spam](http://www.paulgraham.com/spam.html).\n",
    "\n",
    "**What is Bayes?**  \n",
    "Bayes, or Bayes' Theorem, is a different way to assess probability. It considers prior information in order to more accurately assess the situation.\n",
    "\n",
    "**Example:** You are playing roulette.\n",
    "\n",
    "As you approach the table, you see that the last number the ball landed on was Red-3. With a frequentist mindset, you know that the ball is just as likely to land on Red-3 again given that every slot on the wheel has an equal opportunity of 1 in 37.\n",
    "\n",
    "Given that you started believing that the ball can land in each slot with an equal likelihood _and_ that you have only seen one throw previously, you rationally believe that there would be no difference between picking Red a second time now or picking Black -- ideally they would happen with the same likelihood!\n",
    "\n",
    "However, as you sit and watch the roulette table, you begin to notice something strange. The ball is _always_ landing on red. Every single time the ball is thrown, it lands in a red slot. Even though your past beliefs stated that red and black were equally likely, every time it lands in red, you change those beliefs a little more towards a biased roulette table. \n",
    "\n",
    "This is what Bayes is all about — adjusting probabilities as more data is gathered!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the equation for Bayes.  \n",
    "\n",
    "$$P(A \\ | \\ B) = \\frac {P(B \\ | \\ A) \\times P(A)} {P(B)}$$\n",
    "\n",
    "- **$P(A \\ | \\ B)$** : Probability of `Event A` occurring given `Event B` has occurred.\n",
    "- **$P(B \\ | \\ A)$** : Probability of `Event B` occurring given `Event A` has occurred.\n",
    "- **$P(A)$** : Probability of `Event A` occurring.\n",
    "- **$P(B)$** : Probability of `Event B` occurring.\n",
    "\n",
    "\n",
    "Let's train a Naive Bayes classifier on our Yelp reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use default options for CountVectorizer.\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# Create document-term matrices.\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Naive Bayes classifier on our training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialise a Naive Bayes classifier object. We then **fit** our classifier (or **train**) it on our training dataset, which consists of the document-term matrix `X_train_dtm` together with the `t_train`, the labels for each document in our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Naive Bayes to predict the star rating.\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train_dtm, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test our classifier on the testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call our classifier's `predict` method to predict the classifcation (i.e. star rating of the reviews in our testing set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = naive_bayes_classifier.predict(X_test_dtm)\n",
    "y_pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now predict the accuracy of our classifier by calling `scikit-learn`'s `metrics.accuracy_score` method to compare the actual star ratings (`y_test`) to our classifier's predictions, `y_pred_class`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy.\n",
    "print((metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing our classifier to a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the accuracy of our classifier to our basline accuracy, which is a classifier that always predicts the most frequently occuring class in our training set (i.e. 5 star reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate null accuracy.\n",
    "y_test_binary = np.where(y_test==5, 1, 0) # five stars become 1, one stars become 0\n",
    "print('Percent 5 Stars:', y_test_binary.mean())\n",
    "print('Percent 1 Stars:', 1 - y_test_binary.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model predicted ~89% accuracy, which is an improvement over this baseline 81% accuracy (assuming our model always predicts 5 stars)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tfidf'></a>\n",
    "## Term Frequency–Inverse Document Frequency (TF–IDF)\n",
    "\n",
    "While a Count Vectorizer simply totals up the number of times a \"word\" appears in a document, the more complex TF-IDF Vectorizer analyzes the uniqueness of words between documents to find distinguishing characteristics. \n",
    "     \n",
    "Term frequency–inverse document frequency (TF–IDF) computes the \"relative frequency\" with which a word appears in a document, compared to its frequency across all documents.\n",
    "\n",
    "It's more useful than \"term frequency\" for identifying \"important\" words in each document (high frequency in that document, low frequency in other documents).\n",
    "\n",
    "It's used for search-engine scoring, text summarization, and document clustering.\n",
    "\n",
    "**More details:** [TF–IDF is about what matters](http://planspace.org/20150524-tfidf_is_about_what_matters/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example documents\n",
    "simple_train = ['call you tomorrow', 'Call me a cab', 'please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term frequency\n",
    "vect = CountVectorizer()\n",
    "tf = pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document frequency\n",
    "vect = CountVectorizer(binary=True)\n",
    "df = vect.fit_transform(simple_train).toarray().sum(axis=0)\n",
    "pd.DataFrame(df.reshape(1, 6), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term frequency–inverse document frequency (simple version)\n",
    "tf/df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the TF–IDF value, the more \"important\" the word is to that specific document. Here, \"cab\" is the most important and unique word in document 1, while \"please\" is the most important and unique word in document 2. TF–IDF is often used for training as a replacement for word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer\n",
    "vect = TfidfVectorizer()\n",
    "pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
