{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"learning-objectives\"></a>\n",
    "## Learning Objectives\n",
    "*After completing this notebook, you will be able to:*\n",
    "\n",
    "- Define what Pandas is and how it relates to data science.\n",
    "- Manipulate Pandas `DataFrames` and `Series`.\n",
    "- Filter and sort data using Pandas.\n",
    "- Manipulate `DataFrame` columns.\n",
    "- Understand the different kinds of missing data, and know how to handle null and missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents:\n",
    "* [Introduction to Pandas](#pandas-intro)\n",
    "* [Dataframe methods](#dataframe-methods)\n",
    "* [Setting Dataframe values](#setting-values)\n",
    "* [Selecting columns](#selecting-cols)\n",
    "* [Transforming columns](#transforming-cols)\n",
    "* [Selecting rows](#selecting-rows)\n",
    "* [Sorting data](#sorting-data)\n",
    "* [Summary stats](#summary-stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pandas-intro\"></a>\n",
    "\n",
    "# <font color='blue'> Introduction to Pandas\n",
    "\n",
    "Pandas is a Python library that primarily adds two new datatypes to Python: `DataFrame` and `Series`.\n",
    "\n",
    "- A `Series` is a sequence of items, where each item has a unique label (called an `index`).\n",
    "- A `DataFrame` is a table of data. Each row has a unique label (the `row index`), and each column has a unique label (the `column index`).\n",
    "- Note that each column in a `DataFrame` can be considered a `Series` (`Series` index).\n",
    "\n",
    "Behind the scenes, these datatypes use the `numpy` (numerical Python) library. NumPy primarily adds the `ndarray` (n-dimensional array) datatype to Pandas. An `ndarray` is similar to a Python list, in that it stores ordered data. However, it differs in three respects:\n",
    "\n",
    "* Each element has the same datatype (typically fixed-size, e.g., a 32-bit integer).\n",
    "* Elements are stored contiguously (immediately after each other) in memory for fast retrieval.\n",
    "* The total size of an `ndarray` is fixed.\n",
    "\n",
    "Storing `Series` and `DataFrame` data in `ndarray`s makes Pandas faster and uses less memory than standard Python datatypes. Many libraries (such as scikit-learn) accept `ndarray`s as input rather than Pandas datatypes, so we will frequently convert between them.\n",
    "\n",
    "\n",
    "## Using Pandas\n",
    "\n",
    "Pandas is frequently used in data science because it offers a large set of commonly used functions, is relatively fast, and has a large community. Because many data science libraries also use NumPy to manipulate data, you can easily transfer data between libraries (as we will often do in this class!).\n",
    "\n",
    "Pandas is a large library that typically takes a lot of practice to learn. \n",
    "\n",
    "It heavily overrides Python operators, resulting in odd-looking syntax. For example, given a `DataFrame` called `cars` which contains a column `mpg`, we might want to view all cars with mpg over 35. To do this, we might write: `cars[cars['mpg'] > 35]`. \n",
    "\n",
    "In standard Python, this would most likely give a syntax error.  \n",
    "\n",
    "Pandas also highly favors certain patterns of use. \n",
    "\n",
    "For example, looping through a `DataFrame` row by row is highly discouraged. \n",
    "\n",
    "Instead, Pandas favors using **vectorized functions** that operate column by column. (This is because each column is stored separately as an `ndarray`, and NumPy is optimized for operating on `ndarray`s.)\n",
    "\n",
    "Do not be discouraged if Pandas feels overwhelming. Gradually, as you use it, you will become familiar with which methods to use and the \"Pandas way\" of thinking about and manipulating data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='red'> Now you try\n",
    "    \n",
    "Today we'll be working with a dataset on the gender pay gap across companies in the UK. \n",
    "\n",
    "Let's start by downloading the dataset and converting it to a Pandas `DataFrame`.\n",
    "\n",
    "1. Use the `read_csv()` Pandas function to read in two files from the `Data` directory (which is inside the directory this notebook is in). \n",
    "\n",
    "The files have been downloaded from https://gender-pay-gap.service.gov.uk/viewing/download. Here's how you should load each of them in:\n",
    "\n",
    "* `UK Gender Pay Gap Data - 2019 to 2020.csv`; read this in as a DataFrame called `pay_gap_2019_20`\n",
    "    \n",
    "* `UK Gender Pay Gap Data - 2018 to 2019.csv`; read this in as a DataFrame called `pay_gap_2018_19`\n",
    "\n",
    "\n",
    "2. Use the `head` command on `pay_gap_2019_20` to visually inspect the data. What's strange about it? Use `read_csv()` again but try playing around with the `header` parameter (e.g. `read_csv(header=5)`) until the final DataFrame looks right. What does the `header` parameter do?\n",
    "\n",
    "\n",
    "3. Continue to inspect `pay_gap_2019_20` visually and figure out:\n",
    "\n",
    "    \n",
    "* What the data contains\n",
    " \n",
    "* What each column corresponds to\n",
    "    \n",
    "* What each row corresponds to\n",
    "    \n",
    "\n",
    "3. Use `shape` to figure out how many rows are in `pay_gap_2019_20` and `pay_gap_2018_19`. Use the Gender Pay Gap Service website to explain why there's a difference in size\n",
    "\n",
    "\n",
    "\n",
    "4. List as many potential data quality issues as you can in `pay_gap_2019_20`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dataframe-methods\"></a>\n",
    "\n",
    "# <font color='blue'> DataFrame Methods and Attributes\n",
    "\n",
    "We've seen that Pandas `DataFrame` is perhaps the most important class of object in Pandas, and comes with a set of attributes (or properties) and methods that can be applied specifically to Pandas ``DataFrames``. \n",
    "\n",
    "We start by importing ``pandas`` and reading in a CSV file using the ``read_csv`` function. The ``header=2`` parameter specifies that the column names are in row ``2`` of the underlying CSV file.\n",
    "\n",
    "We preview the first five rows of the ``DataFrame`` using the ``head`` method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20 = pd.read_csv('./data/UK Gender Pay Gap Data - 2019 to 2020.csv')\n",
    "pay_gap_2019_20.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the index, which is a numbering system that labels each row with a unique number according to its position in the DataFrame (like indexing in a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also quickly access the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``shape`` attribute is a good way of figuring out how big our dataset is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that our ``DataFrame`` is the correct type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pay_gap_2019_20)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the types of data in individual columns. **But first, we need to deliberately engineer a problem with our data by runnng the cell below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20 = pay_gap_2019_20.astype({'DiffMedianHourlyPercent': 'str',\n",
    "                                         'DiffMeanBonusPercent': 'str',\n",
    "                                         'DiffMeanHourlyPercent':'str'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the types using `dtypes()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most of the columns in our dataset are ``float64``, i.e. floating point or **decimal** numbers.\n",
    "\n",
    "But we can also see that the `DiffMeanHourlyPercent`, `DiffMedianHourlyPercent` and `DiffMeanBonusPercent` columns are **not** a numeric type. If a column in a DataFrame contains a mix of types, Pandas labels its type as `object`.\n",
    "\n",
    "Since we want Pandas to treat these columns as numeric columns, we need to convert it using the `to_numeric` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20['DiffMeanHourlyPercent'] = pd.to_numeric(pay_gap_2019_20['DiffMeanHourlyPercent'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we run `dtypes` again, we can see the `DiffMeanHourlyPercent` column has a numeric type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That leaves the `DiffMedianHourlyPercent` and `DiffMeanBonusPercent` columns to convert. Instead of running `to_numeric()` two more times, it's more efficient to convert multiple columns to different types using the `astype` method.\n",
    "\n",
    "**Note that the information we give Pandas about which columns to convert, and which types to convert them to, is formatted as a dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20 = pay_gap_2019_20.astype({'DiffMedianHourlyPercent': 'float64',\n",
    "                                         'DiffMeanBonusPercent': 'float64'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running `dtypes` a final time, we see that all the columns in our DataFrame are of the correct type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setting-values\"></a>\n",
    "\n",
    "# <font color='blue'> Setting values in a DataFrame\n",
    "\n",
    "To change the value of a single element in a DataFrame, we use the `at` method.\n",
    "\n",
    "We pass it the position of the element we want to set the value of, in the format `[index,column_name]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.at[0,'EmployerName'] = 'test value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"selecting-cols\"></a>\n",
    "\n",
    "# <font color='blue'> Selecting columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas DataFrames have structural similarities with Python-style lists and dictionaries. We can select, or extract, columns from a `DataFrame` using column names.\n",
    "\n",
    "\n",
    "\n",
    "In the example below, we select a column of data using the name of the column in a similar manner to how we select a dictionary value with the dictionary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pay_gap_2019_20['EmployerName']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a Pandas **series**. We can think of this as being the Pandas equivalent of a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pay_gap_2019_20['EmployerName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also select a single column using this syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pay_gap_2019_20[['EmployerName']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pay_gap_2019_20[['EmployerName']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select multiple columns using this syntax too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20[['EmployerName','Address']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neater way of doing it could be using this syntax, which does exactly the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns_to_select = ['EmployerName','Address']  \n",
    "pay_gap_2019_20[columns_to_select]            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"transforming-cols\"></a>\n",
    "\n",
    "# <font color='blue'> Transforming columns\n",
    "    \n",
    "Once we've selected columns, we can perform transformations on them (e.g converting an entire column to lowercase) or calculations with them (e.g. adding two columns together to create a new column)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing column names\n",
    "\n",
    "There are a few different ways to change column names. \n",
    "\n",
    "### Renaming individual columns\n",
    "\n",
    "Individual column names can be changed like this. We could add as many columns as we wanted to the dictionary below, in the format `{'old_column_name':'new_column_name'}`\n",
    "\n",
    "`rename` is by default **not** an **in place** method, i.e. it doesn't change the underlying DataFrame. In order to make methods **in place** we need to add an extra input to the `rename` method; `inplace=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.rename(columns={'Address':'EmployerAddress'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the column has been renamed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to rename **all** the columns in a DataFrame using the syntax\n",
    "\n",
    "``DataFrame.columns = [full list of new column names]``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <font color='red'> Now you try\n",
    "    \n",
    "1. Use pandas to read in the file `country_demographics.csv` from the `data` folder, as a DataFrame called `county_data`. \n",
    "\n",
    "\n",
    "2. Visually inspect the DataFrame. What's the main problem with this data?\n",
    "\n",
    "\n",
    "3. Luckily, we have a data dictionary that can help us below! Below is some pre-written code that loads in our data dictionary as a DataFrame, then converts it to a dictionary.\n",
    "\n",
    "You'll see that the dictionary translates the column names in `county_data` to longer names. Use this dictionary to rename the columns in `county_data` formatted in **snake case** i.e. so each column is in lowercase, with words separated by an underscore. For example, the column titled `PST045214` in `county_demographics` should be renamed as `population_2014_estimate`.\n",
    "\n",
    "**There are more and less efficient ways of completing this task. Try to go for the method that's smart but lazy. Hint: it might involve a `for` loop around the dictionary, and the `.lower()` and `join()` string methods...**\n",
    "\n",
    "4. Figure out how to use the `to_csv()` method to save the DataFrame with renamed columns as a file called `county_data_clean.csv`. Make sure the file is saved in the `data` directory in this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'column_name': 'PST045214', 'description': 'Population 2014 estimate'},\n",
       " {'column_name': 'PST040210',\n",
       "  'description': 'Population 2010 April 1 estimates base'},\n",
       " {'column_name': 'PST120214',\n",
       "  'description': 'Population percent change  April 1 2010 to July 1 2014'},\n",
       " {'column_name': 'POP010210', 'description': 'Population 2010'},\n",
       " {'column_name': 'AGE135214',\n",
       "  'description': 'Persons under 5 years percent 2014'},\n",
       " {'column_name': 'AGE295214',\n",
       "  'description': 'Persons under 18 years percent 2014'},\n",
       " {'column_name': 'AGE775214',\n",
       "  'description': 'Persons 65 years and over percent 2014'},\n",
       " {'column_name': 'SEX255214', 'description': 'Female persons percent 2014'},\n",
       " {'column_name': 'RHI125214', 'description': 'White alone percent 2014'},\n",
       " {'column_name': 'RHI225214',\n",
       "  'description': 'Black or African American alone percent 2014'},\n",
       " {'column_name': 'RHI325214',\n",
       "  'description': 'American Indian and Alaska Native alone percent 2014'},\n",
       " {'column_name': 'RHI425214', 'description': 'Asian alone percent 2014'},\n",
       " {'column_name': 'RHI525214',\n",
       "  'description': 'Native Hawaiian and Other Pacific Islander alone percent 2014'},\n",
       " {'column_name': 'RHI625214', 'description': 'Two or More Races percent 2014'},\n",
       " {'column_name': 'RHI725214',\n",
       "  'description': 'Hispanic or Latino percent 2014'},\n",
       " {'column_name': 'RHI825214',\n",
       "  'description': 'White alone not Hispanic or Latino percent 2014'},\n",
       " {'column_name': 'POP715213',\n",
       "  'description': 'Living in same house 1 year & over percent 20092013'},\n",
       " {'column_name': 'POP645213',\n",
       "  'description': 'Foreign born persons percent 20092013'},\n",
       " {'column_name': 'POP815213',\n",
       "  'description': 'Language other than English spoken at home pct age 5plus 20092013'},\n",
       " {'column_name': 'EDU635213',\n",
       "  'description': 'High school graduate or higher percent of persons age 25plus 20092013'},\n",
       " {'column_name': 'EDU685213',\n",
       "  'description': \"Bachelor's degree or higher percent of persons age 25plus 20092013\"},\n",
       " {'column_name': 'VET605213', 'description': 'Veterans 20092013'},\n",
       " {'column_name': 'LFE305213',\n",
       "  'description': 'Mean travel time to work minutes workers age 16plus 20092013'},\n",
       " {'column_name': 'HSG010214', 'description': 'Housing units 2014'},\n",
       " {'column_name': 'HSG445213', 'description': 'Homeownership rate 20092013'},\n",
       " {'column_name': 'HSG096213',\n",
       "  'description': 'Housing units in multiunit structures percent 20092013'},\n",
       " {'column_name': 'HSG495213',\n",
       "  'description': 'Median value of owneroccupied housing units 20092013'},\n",
       " {'column_name': 'HSD410213', 'description': 'Households 20092013'},\n",
       " {'column_name': 'HSD310213', 'description': 'Persons per household 20092013'},\n",
       " {'column_name': 'INC910213',\n",
       "  'description': 'Per capita money income in past 12 months 2013 dollars 20092013'},\n",
       " {'column_name': 'INC110213',\n",
       "  'description': 'Median household income 20092013'},\n",
       " {'column_name': 'PVY020213',\n",
       "  'description': 'Persons below poverty level percent 20092013'},\n",
       " {'column_name': 'BZA010213',\n",
       "  'description': 'Private nonfarm establishments 2013'},\n",
       " {'column_name': 'BZA110213',\n",
       "  'description': 'Private nonfarm employment  2013'},\n",
       " {'column_name': 'BZA115213',\n",
       "  'description': 'Private nonfarm employment percent change 20122013'},\n",
       " {'column_name': 'NES010213',\n",
       "  'description': 'Nonemployer establishments 2013'},\n",
       " {'column_name': 'SBO001207', 'description': 'Total number of firms 2007'},\n",
       " {'column_name': 'SBO315207', 'description': 'Blackowned firms percent 2007'},\n",
       " {'column_name': 'SBO115207',\n",
       "  'description': 'American Indian and Alaska Nativeowned firms percent 2007'},\n",
       " {'column_name': 'SBO215207', 'description': 'Asianowned firms percent 2007'},\n",
       " {'column_name': 'SBO515207',\n",
       "  'description': 'Native Hawaiian and Other Pacific Islanderowned firms percent 2007'},\n",
       " {'column_name': 'SBO415207',\n",
       "  'description': 'Hispanicowned firms percent 2007'},\n",
       " {'column_name': 'SBO015207', 'description': 'Womenowned firms percent 2007'},\n",
       " {'column_name': 'MAN450207',\n",
       "  'description': 'Manufacturers shipments 2007 $1000'},\n",
       " {'column_name': 'WTN220207',\n",
       "  'description': 'Merchant wholesaler sales 2007 $1000'},\n",
       " {'column_name': 'RTN130207',\n",
       "  'description': 'Retail sales 2007 thousands dollars'},\n",
       " {'column_name': 'RTN131207', 'description': 'Retail sales per capita 2007'},\n",
       " {'column_name': 'AFN120207',\n",
       "  'description': 'Accommodation and food services sales 2007 $1000'},\n",
       " {'column_name': 'BPS030214', 'description': 'Building permits 2014'},\n",
       " {'column_name': 'LND110210', 'description': 'Land area in square miles 2010'},\n",
       " {'column_name': 'POP060210',\n",
       "  'description': 'Population per square mile 2010'}]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "county_data_dictionary = pd.read_csv('./data/county_facts_dictionary.csv').to_dict('records')\n",
    "county_data_dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new columns\n",
    "\n",
    "We can create new columns by performing calculations on existing columns. Let's say we want to create a new column that gives the Difference in Mean Hourly Pay as a proportion rather than a percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20['DiffMeanHourlyProportion'] = pay_gap_2019_20['DiffMeanHourlyPercent']/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the new column has been created at the end of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create 'blank' new columns. We might want to do this if we're going to fill an empty column with new data.\n",
    "\n",
    "We can create a column of empty strings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20['empty_string_column'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a column full of zeroes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20['zeroes_column'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a column that goes up in steps of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20['counting_column'] = range(0,pay_gap_2019_20.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new columns have been created at the end of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing columns\n",
    "\n",
    "We can use the `drop` method to do this. Once again, unless we specify that the method is `inplace` the underlying DataFrame won't be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.drop(columns=['DateSubmitted','DueDate'],inplace=True)\n",
    "pay_gap_2019_20.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying functions to columns\n",
    "\n",
    "Sometimes we'll want to perform a calculation or operation on each row of a DataFrame column. There are a few different ways to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorised functions\n",
    "\n",
    "In Pandas it's discouraged to loop through all the rows in a DataFrame, applying a function or operation to each row. \n",
    "\n",
    "Vectorised functions, which quickly apply a function to an entire column without having to explicitly write a loop, are much faster and more efficient. \n",
    "\n",
    "Here are some examples.\n",
    "\n",
    "We can convert columns to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20['EmployerName'] = pay_gap_2019_20['EmployerName'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can replace strings. This can be used to remove strings, too by replacing them with a blank space or `''`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20['EmployerName'] = pay_gap_2019_20['EmployerName'].str.replace('limited','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `apply` method and lambda functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we'll want to perform more complex operations on each row of a DataFrame column. \n",
    "\n",
    "If there isn't a built-in DataFrame method for what we want to do, we can write our own function and then apply it to each row of the column using the `.apply()` method.\n",
    "\n",
    "Let's say I want to create a new column that contains the first name of the `ResponsiblePerson` in the data.\n",
    "\n",
    "We want to apply the `split()` function to each row of the `ResponsiblePerson` column, take the first value of the result and record this in a new column.\n",
    "\n",
    "Let's remind ourselves of what `split()` does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'hello world'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'hello-world'.split('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'0 to 100'.split('to')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `apply` DataFrame method to apply `split()` to each row of the `ResponsiblePerson` column, creating a new column called `ResponsiblePersonFirstName`.\n",
    "\n",
    "The input to `apply` is the function we want to apply to each row. We can define the function elsewhere and call it inside `apply` via a **lambda** function. This is a quick, throwaway method of writing a function; you don't need to worry too much about these just yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_firstname(x):\n",
    "    \n",
    "    return  x.split()[0]\n",
    "\n",
    "pay_gap_2019_20['ResponsiblePersonFirstName'] = pay_gap_2019_20['ResponsiblePerson'].apply(lambda x: get_firstname(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `for` loops (the ugly way to do it)\n",
    "\n",
    "If you really need to, it's also possible to iterate through every row of a DataFrame using the `iterrows` method.\n",
    "\n",
    "Let's take a look at how it works. It's a bit like iterating through the elements of a list, except we're iterating through the rows of the DataFrame; each iteration changes the value of `row` (which is the entire current row of the DataFrame) and `idx` (which is the index of the current row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, row in pay_gap_2019_20.iterrows():\n",
    "    \n",
    "    print(idx)\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to use this syntax to pick out the **last** names of the people in the `ResponsiblePerson` column. We could do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in pay_gap_2019_20.iterrows():\n",
    "    \n",
    "    split_name = row['ResponsiblePerson'].split()\n",
    "    \n",
    "    if len(split_name)>=2:\n",
    "        pay_gap_2019_20.at[idx,'ResponsiblePersonLastName'] = split_name[1]\n",
    "    else:\n",
    "        pay_gap_2019_20.at[idx,'ResponsiblePersonLastName'] = 'None'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='red'> Now you try\n",
    "    \n",
    "1. Using **two** existing columns in `county_data`, work out an estimate of the area of each county in square miles. Store these estimates in a new column called `approximate_area`. What are the units? \n",
    "\n",
    "\n",
    "2. Create a new column that calculates the **difference** or **error** between your estimates of area, and the actual values in the `land_area_in_square_miles_2010` column.\n",
    "\n",
    "\n",
    "3. Figure out how to use the `sum()` method to work out the total area of all counties in the dataset.\n",
    "\n",
    "\n",
    "4. Figure out how to use the `mean()` method to work out the mean population across all counties.\n",
    "\n",
    "\n",
    "5. Calculate a new column called `predicted_population_2018` that estimates the 2018 population of each county, assuming the same growth rate in population seen between 2010 and 2014.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"selecting-rows\"></a>\n",
    "\n",
    "# <font color='blue'> Selecting rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting rows by index\n",
    "\n",
    "We can use the `loc` command to pick out a specific row of a DataFrame.\n",
    "\n",
    "We use the syntax `loc[a,b]` where `a` is the index of the row we want to access, and `b` is the name of the column. \n",
    "\n",
    "As with lists, `:` means 'give me everything' so in this example below, we're accessing the **first** row of data and **all** the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.loc[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify a **range** of rows we want to extract. This gives us rows **0** to **2** **inclusive of row 5** and all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.loc[0:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify rows and single columns, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.loc[0:2,'EmployerName']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the rows we want plus the list of columns we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.loc[0:2,['EmployerName','EmployerAddress']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, the rows we want and the **range** of columns we want (notice the `:` operator again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.loc[0:2,'EmployerName':'SicCodes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting rows using logical tests\n",
    "\n",
    "Often we won't know the exact index of the row we're looking for. \n",
    "\n",
    "Maybe we want to find all the rows where the `DiffMedianHourlyPercent` is greater than 10%.\n",
    "\n",
    "We start by writing a **filter** or a logical test that will be `True` for the rows we're interested in. \n",
    "\n",
    "We're interested in the `DiffMedianHourlyPercent` column so our filter looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_filter = pay_gap_2019_20['DiffMedianHourlyPercent']>10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we inspect this filter, we can see it's a long list of `True` and `False` values; the value of the filter is `True` for rows that pass the logical test and `False` for rows that don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we **apply** our filter to our DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20[pay_gap_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write and apply our filter in a single step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20[pay_gap_2019_20['DiffMedianHourlyPercent']>10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to combine logical tests using `and` and `or` operators. For example, to find all rows where `DiffMedianHourlyPercent` is greater than 10% **and** `DiffMeanHourlyPercent` is greater than 10%, we can write:\n",
    "\n",
    "**Note that the `and` operator here is written as `&`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_filter_2 = (pay_gap_2019_20['DiffMedianHourlyPercent']>10) & (pay_gap_2019_20['DiffMeanHourlyPercent']>10)\n",
    "\n",
    "pay_gap_2019_20[pay_gap_filter_2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, to find all rows where `DiffMedianHourlyPercent` is greater than 10% **or** `DiffMeanHourlyPercent` is greater than 10%, we can write:\n",
    "\n",
    "**Note that the `or` operator here is written as `|`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_filter_3 = (pay_gap_2019_20['DiffMedianHourlyPercent']>10) | (pay_gap_2019_20['DiffMeanHourlyPercent']>10)\n",
    "\n",
    "pay_gap_2019_20[pay_gap_filter_3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the ``str.contains()`` method to find all rows that contain a particular string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_filter_4 = pay_gap_2019_20['EmployerName'].str.lower().str.contains('school')\n",
    "\n",
    "pay_gap_2019_20[pay_gap_filter_4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='red'> Now you try\n",
    "    \n",
    "1. How many counties are in Alabama?\n",
    "\n",
    "\n",
    "2. What's the approximate total **number** of Hispanic/Latino people in Texas?\n",
    "\n",
    "\n",
    "3. What's the mean per capita income in Kansas? \n",
    "\n",
    "\n",
    "4. What proportion of people in the USA live in counties where the median household income is less than $30,000?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sorting-data\"></a>\n",
    "\n",
    "# <font color='blue'> Sorting data\n",
    "    \n",
    "It's easy to sort data in ascending/descending order according to a particular column. We do this using the `sort_values` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pay_gap_2019_20.sort_values(by='DiffMedianHourlyPercent',ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='red'> Now you try\n",
    "    \n",
    "1. Which county has the lowest proportion of people with a bachelors degree?\n",
    "\n",
    "\n",
    "2. Which county has the highest proportion of people living below the poverty level?\n",
    "\n",
    "\n",
    "3. What are the top five counties with the highest proportion of old people?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary-stats\"></a>\n",
    "\n",
    "# <font color='blue'> Summary statistics\n",
    "\n",
    "Pandas has a bunch of built-in methods to quickly summarize your data and provide you with a quick general understanding. \n",
    "\n",
    "The ``describe`` method gives summary statistics for the numeric columns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.describe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to get summary statistics for all columns, including non-numeric ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"value-counts\"></a>\n",
    "# <font color='blue'> Getting value counts\n",
    "\n",
    "Sometimes we might want to see the breakdown of different values in a column. This is easy with the `value_counts` function.\n",
    "\n",
    "In our gender dataset, let's check the breakdown of company sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pay_gap_2019_20['EmployerSize'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the ``plot`` method gives a visual representation of value counts, provided we import `matplotlib` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pay_gap_2019_20['EmployerSize'].value_counts().plot(kind='bar');\n",
    "plt.xlabel('Company size');\n",
    "plt.ylabel('Count');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"groupby\"></a>\n",
    "# <font color='blue'> Grouping data\n",
    "\n",
    "Sometimes we might want a more detailed breakdown using more than one column. To understand this a bit better, let's read in a CSV containing our stop and search data from last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_range</th>\n",
       "      <th>outcome</th>\n",
       "      <th>involved_person</th>\n",
       "      <th>self_defined_ethnicity</th>\n",
       "      <th>gender</th>\n",
       "      <th>legislation</th>\n",
       "      <th>outcome_linked_to_object_of_search</th>\n",
       "      <th>datetime</th>\n",
       "      <th>removal_of_more_than_outer_clothing</th>\n",
       "      <th>outcome_object</th>\n",
       "      <th>location</th>\n",
       "      <th>operation</th>\n",
       "      <th>officer_defined_ethnicity</th>\n",
       "      <th>type</th>\n",
       "      <th>operation_name</th>\n",
       "      <th>object_of_search</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>25-34</td>\n",
       "      <td>A no further action disposal</td>\n",
       "      <td>True</td>\n",
       "      <td>Asian/Asian British - Any other Asian background</td>\n",
       "      <td>Male</td>\n",
       "      <td>Misuse of Drugs Act 1971 (section 23)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-05-01T14:09:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'id': 'bu-no-further-action', 'name': 'A no f...</td>\n",
       "      <td>{'latitude': '51.530565', 'street': {'id': 960...</td>\n",
       "      <td>False</td>\n",
       "      <td>Other</td>\n",
       "      <td>Person search</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Controlled drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>over 34</td>\n",
       "      <td>Summons / charged by post</td>\n",
       "      <td>True</td>\n",
       "      <td>Black/African/Caribbean/Black British - African</td>\n",
       "      <td>Male</td>\n",
       "      <td>Misuse of Drugs Act 1971 (section 23)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-05-01T14:05:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'id': 'bu-summons', 'name': 'Summons / charge...</td>\n",
       "      <td>{'latitude': '51.530565', 'street': {'id': 960...</td>\n",
       "      <td>False</td>\n",
       "      <td>Black</td>\n",
       "      <td>Person search</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Controlled drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>over 34</td>\n",
       "      <td>A no further action disposal</td>\n",
       "      <td>True</td>\n",
       "      <td>Asian/Asian British - Pakistani</td>\n",
       "      <td>Male</td>\n",
       "      <td>Misuse of Drugs Act 1971 (section 23)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-05-01T15:14:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'id': 'bu-no-further-action', 'name': 'A no f...</td>\n",
       "      <td>{'latitude': '51.524521', 'street': {'id': 960...</td>\n",
       "      <td>False</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Person search</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Controlled drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>18-24</td>\n",
       "      <td>A no further action disposal</td>\n",
       "      <td>True</td>\n",
       "      <td>Black/African/Caribbean/Black British - Any ot...</td>\n",
       "      <td>Male</td>\n",
       "      <td>Misuse of Drugs Act 1971 (section 23)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-05-01T17:05:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'id': 'bu-no-further-action', 'name': 'A no f...</td>\n",
       "      <td>{'latitude': '51.530361', 'street': {'id': 960...</td>\n",
       "      <td>False</td>\n",
       "      <td>Black</td>\n",
       "      <td>Person search</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Controlled drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>25-34</td>\n",
       "      <td>A no further action disposal</td>\n",
       "      <td>True</td>\n",
       "      <td>White - English/Welsh/Scottish/Northern Irish/...</td>\n",
       "      <td>Male</td>\n",
       "      <td>Misuse of Drugs Act 1971 (section 23)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-05-01T15:14:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'id': 'bu-no-further-action', 'name': 'A no f...</td>\n",
       "      <td>{'latitude': '51.524521', 'street': {'id': 960...</td>\n",
       "      <td>False</td>\n",
       "      <td>White</td>\n",
       "      <td>Person search</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Controlled drugs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  age_range                       outcome  involved_person  \\\n",
       "0     25-34  A no further action disposal             True   \n",
       "1   over 34     Summons / charged by post             True   \n",
       "2   over 34  A no further action disposal             True   \n",
       "3     18-24  A no further action disposal             True   \n",
       "4     25-34  A no further action disposal             True   \n",
       "\n",
       "                              self_defined_ethnicity gender  \\\n",
       "0   Asian/Asian British - Any other Asian background   Male   \n",
       "1    Black/African/Caribbean/Black British - African   Male   \n",
       "2                    Asian/Asian British - Pakistani   Male   \n",
       "3  Black/African/Caribbean/Black British - Any ot...   Male   \n",
       "4  White - English/Welsh/Scottish/Northern Irish/...   Male   \n",
       "\n",
       "                             legislation outcome_linked_to_object_of_search  \\\n",
       "0  Misuse of Drugs Act 1971 (section 23)                                NaN   \n",
       "1  Misuse of Drugs Act 1971 (section 23)                                NaN   \n",
       "2  Misuse of Drugs Act 1971 (section 23)                                NaN   \n",
       "3  Misuse of Drugs Act 1971 (section 23)                                NaN   \n",
       "4  Misuse of Drugs Act 1971 (section 23)                                NaN   \n",
       "\n",
       "                    datetime removal_of_more_than_outer_clothing  \\\n",
       "0  2019-05-01T14:09:00+00:00                                 NaN   \n",
       "1  2019-05-01T14:05:00+00:00                                 NaN   \n",
       "2  2019-05-01T15:14:00+00:00                                 NaN   \n",
       "3  2019-05-01T17:05:00+00:00                                 NaN   \n",
       "4  2019-05-01T15:14:00+00:00                                 NaN   \n",
       "\n",
       "                                      outcome_object  \\\n",
       "0  {'id': 'bu-no-further-action', 'name': 'A no f...   \n",
       "1  {'id': 'bu-summons', 'name': 'Summons / charge...   \n",
       "2  {'id': 'bu-no-further-action', 'name': 'A no f...   \n",
       "3  {'id': 'bu-no-further-action', 'name': 'A no f...   \n",
       "4  {'id': 'bu-no-further-action', 'name': 'A no f...   \n",
       "\n",
       "                                            location operation  \\\n",
       "0  {'latitude': '51.530565', 'street': {'id': 960...     False   \n",
       "1  {'latitude': '51.530565', 'street': {'id': 960...     False   \n",
       "2  {'latitude': '51.524521', 'street': {'id': 960...     False   \n",
       "3  {'latitude': '51.530361', 'street': {'id': 960...     False   \n",
       "4  {'latitude': '51.524521', 'street': {'id': 960...     False   \n",
       "\n",
       "  officer_defined_ethnicity           type  operation_name  object_of_search  \n",
       "0                     Other  Person search             NaN  Controlled drugs  \n",
       "1                     Black  Person search             NaN  Controlled drugs  \n",
       "2                     Asian  Person search             NaN  Controlled drugs  \n",
       "3                     Black  Person search             NaN  Controlled drugs  \n",
       "4                     White  Person search             NaN  Controlled drugs  "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "police_dataframe = pd.read_csv('./data/stop_and_search_may_2019_london.csv')\n",
    "police_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `groupby` method to get a breakdown of the outcomes of searches, by gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender  outcome                        \n",
       "Female  A no further action disposal        37\n",
       "        Arrest                               5\n",
       "        Community resolution                 2\n",
       "Male    A no further action disposal       370\n",
       "        Arrest                              57\n",
       "        Community resolution                43\n",
       "        Penalty Notice for Disorder          9\n",
       "        Caution (simple or conditional)      1\n",
       "        Khat or Cannabis warning             1\n",
       "        Summons / charged by post            1\n",
       "Name: outcome, dtype: int64"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "police_dataframe.groupby('gender')['outcome'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `normalize` input to `value_counts` to transform the values into proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender  outcome                        \n",
       "Female  A no further action disposal       0.840909\n",
       "        Arrest                             0.113636\n",
       "        Community resolution               0.045455\n",
       "Male    A no further action disposal       0.767635\n",
       "        Arrest                             0.118257\n",
       "        Community resolution               0.089212\n",
       "        Penalty Notice for Disorder        0.018672\n",
       "        Caution (simple or conditional)    0.002075\n",
       "        Khat or Cannabis warning           0.002075\n",
       "        Summons / charged by post          0.002075\n",
       "Name: outcome, dtype: float64"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "police_dataframe.groupby('gender')['outcome'].value_counts('normalize')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='red'> Now you try\n",
    "    \n",
    "In the Police dataset, can you use `value_counts` to get breakdowns of:\n",
    "\n",
    "* The ethnicities of people who were stopped and searched\n",
    "\n",
    "\n",
    "* The age brackets of people who were stopped and searched\n",
    "\n",
    "\n",
    "* The 'object of search' (i.e. what officers were looking for)\n",
    "\n",
    "\n",
    "* The outcomes of the search\n",
    "\n",
    "\n",
    "* The proportion of people stopped who were men\n",
    "\n",
    "\n",
    "Use a combination of `value_counts`, `groupby` and filtering skills to calculate:\n",
    "\n",
    "* The proportion of people stopped who were 18-24 years old, broken down by gender\n",
    "\n",
    "\n",
    "* The breakdown of reasons people were stopped, broken down by ethnicity\n",
    "\n",
    "\n",
    "* The breakdown of the ages of people who were stopped, broken down by gender\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <font color='red'> Now you try\n",
    "    \n",
    "1. What's the breakdown of states, in counties where the median household income is equal to or below the **lower quartile value**? (hint: `describe()` will get you lower and upper quartiles)\n",
    "\n",
    "\n",
    "2. How many people are living in counties where the median household income is equal to or below the lower quartile value?\n",
    "\n",
    "\n",
    "3. What's the breakdown of states, in counties where the median household income is equal to or above the **upper quartile value**?\n",
    "\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"value-counts\"></a>\n",
    "# <font color='blue'> Handling missing values\n",
    "    \n",
    "Sometimes, values will be missing from the source data or as a byproduct of manipulations. It is very important to detect missing data. Missing data can:\n",
    "\n",
    "- Make the entire row ineligible to be training data for a model.\n",
    "- Hint at data-collection errors.\n",
    "- Indicate improper conversion or manipulation.\n",
    "- Actually not be missing — it sometimes means \"zero,\" \"false,\" \"not applicable,\" or \"entered an empty string.\"\n",
    "\n",
    "For example, a `.csv` file might have a missing value in some data fields:\n",
    "\n",
    "```\n",
    "tool_name,material,cost\n",
    "hammer,wood,8\n",
    "chainsaw,,\n",
    "wrench,metal,5\n",
    "```\n",
    "\n",
    "When this data is imported, \"null\" values will be stored in the second row (in the \"material\" and \"cost\" columns).\n",
    "\n",
    "In Pandas, a \"null\" value is either `None` or `np.NaN` (Not a Number). \n",
    "\n",
    "Many fixed-size numeric datatypes (such as integers) do not have a way of representing `np.NaN`. So, numeric columns will be promoted to floating-point datatypes that do support it. For example, when importing the `.csv` file above:\n",
    "\n",
    "**For the second row:** `None` will be stored in the \"material\" column and `np.NaN` will be stored in the \"cost\" column. The entire \"cost\" column (stored as a single `ndarray`) must be stored as floating-point values to accommodate the `np.NaN`, even though an integer `8` is in the first row.\n",
    "\n",
    "Let's check our gender pay gap dataset for missing values.\n",
    "\n",
    "We can do this using the `isnull()` method and summing up the values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose to drop rows containing ``NaN`` values, or fill in ``NaN`` values with a string, float or other element of our choice. \n",
    "\n",
    "Be careful when doing either of these things; you could end up unintentionally removing rows, or filling in values that don't make sense or aren't accurate.\n",
    "\n",
    "In this case, it would be important to clarify whether a ``NaN`` value in a particular column means the amount is zero, or whether it means the amount is unknown.\n",
    "\n",
    "We can **fill in** NaN values with a value of our choice using `fillna()`. For example, it makes sense to fill in `CompanyLinkToGPGInfo` with a string like 'no URL provided'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20['CompanyLinkToGPGInfo'].fillna('No URL',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that this column no longer has any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20['CompanyLinkToGPGInfo'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to **drop** rows where there is no company number provided, since this means we won't be able to look up the company on Companies House."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.dropna(subset=['CompanyNumber'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can now see that there are no missing values in the `CompanyNumber` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_gap_2019_20.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='red'> Now you try\n",
    "\n",
    "We'll be working with Twitter's election integrity dataset from October 2018, which consists of ~3million tweets from accounts suspected to be associated with overseas bot accounts.\n",
    "\n",
    "Some of the code in these exercises is boilerplated (i.e. written for you), with gaps for you to fill in. \n",
    "\n",
    "Instructions are provided in the comments where this is the case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Read in the data\n",
    "\n",
    "Visit this URL, and enter your email address to access information about the datasets. Read a bit about which datasets are available by clicking on : https://transparency.twitter.com/en/information-operations.html\n",
    "\n",
    "In particular, read the ``Readme`` file to understand the variables contained in each dataset.\n",
    "\n",
    "\n",
    "We'll be downloading the tweets associated with the **Iran (October 2018) – 770 accounts** dataset. \n",
    "\n",
    "To do this, you should:\n",
    "\n",
    "* Click on this URL, and download the resulting ZIP file to your computer.\n",
    "https://storage.googleapis.com/twitter-election-integrity/hashed/iranian/iranian_tweets_csv_hashed.zip\n",
    "\n",
    "\n",
    "* Unzip the contents of the file to the ``data`` directory inside the same directory as this notebook. \n",
    "\n",
    "The result should be a file called ``iranian_tweets_csv_hashed.csv`` in location ``./data/`` relative to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's read in the dataset- this might take a while because it's enormous!\n",
    "# fill in the filepath here\n",
    "tweet_df = pd.read_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the first 10 rows of the dataset using ``head``. What does each row correspond to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Inspect the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the shape of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ``value_counts()`` to get summary counts for the language of the tweets and the location of the account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ``describe`` to get a summary of the numeric columns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Filter out non-UK based accounts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the data so that only tweets where ``user_reported_location`` is ``United Kingdom`` are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Remove columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only the following columns:\n",
    "\n",
    "``tweetid,\n",
    "follower_count,\n",
    "user_screen_name,\n",
    "following_count,\n",
    "account_creation_date,\n",
    "tweet_text,\n",
    "tweet_time,\n",
    "like_count,\n",
    "retweet_count``\n",
    "\n",
    "\n",
    "It's more efficient to do this by selecting columns using ``my_df[['col1','col2','col3']]`` notation rather than using ``my_df.drop(columns=[])`` since we want to drop many more columns than we want to keep.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) How many unique accounts were purportedly tweeting from the UK?\n",
    "\n",
    "`value_counts()` will come in useful here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Convert columns to the pandas ``datetime`` type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the ``pd.to_datetime()`` function to convert the ``account_creation_date`` and ``tweet_time`` columns into the type ``datetime``. This is a type in ``pandas`` that allows dates to be treated like timestamps, so we can search by date, sort in chronological order etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ``dtypes`` to confirm that these columns are now ``datetime`` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) When was this account tweeting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the ``hist()`` method to get a quick visualisation of the distribution of ``tweet_time``s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Did the account get many likes or retweets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ``value_counts()`` and ``describe()`` on the ``like_count`` and ``retweet_count`` columns to gauge how successful you think this account was at propagating information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) What was this account tweeting about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start to explore the contents of each tweet using some basic word count methods. This is much less advanced than the more involved natural language processing methods we'll be using later on in the course, but is a good start. \n",
    "\n",
    "Begin by using the ``str.lower()`` method to convert the ``tweet_text`` column to lowercase so we don't have to worry about case sensitivity, and also using ``str.replace()`` together with a **regular expression** to catch **all puncuation marks** and replace them with an empty string; this is the same thing as stripping out all punctuation.\n",
    "\n",
    "Your code will look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['tweet_text'] = tweet_df['tweet_text'].str.lower().str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use filtering and ``.str.contains()`` to find out what **percentage** of the tweets mention the following terms (remember we've converted everything to lowercase so your search terms need to be lowercase as well):\n",
    "\n",
    "* obama\n",
    "* brexit\n",
    "* trump\n",
    "* syria\n",
    "* iran\n",
    "* uk \n",
    "* russia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "# Summary\n",
    "\n",
    "Believe it or not, we've only barely touched the surface of everything that Pandas offers. Don't worry if you don't remember most of it — for now, just knowing what exists is key. Remember that the more you use Pandas to manipulate data, the more of these functions you will take interest in, look up, and remember.\n",
    "\n",
    "In this notebook, the most important things to familiarize yourself with are the basics:\n",
    "- Manipulating `DataFrames` and `Series`\n",
    "- Filtering columns and rows\n",
    "- Handling missing values\n",
    "- Value counts and Groupby"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
